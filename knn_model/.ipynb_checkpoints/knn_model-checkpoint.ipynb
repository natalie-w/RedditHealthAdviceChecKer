{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4bbd4be",
   "metadata": {},
   "source": [
    "# Reddit HealthAdviceChecKer Bot Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c812088",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bb34f8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package genesis to\n",
      "[nltk_data]     /Users/gwenythportillowightman/nltk_data...\n",
      "[nltk_data]   Package genesis is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gwenythportillowightman/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/gwenythportillowightman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/gwenythportillowightman/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import genesis\n",
    "nltk.download('genesis')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "genesis_ic = wn.ic(genesis, False, 0.0)\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667ae606",
   "metadata": {},
   "source": [
    "## Filtering the dataset to health claims only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d1b29755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format tags columns in df\n",
    "\n",
    "def format_tags(df):\n",
    "    tags = []\n",
    "    tag_lists = []\n",
    "\n",
    "    for subjects in df.subjects:\n",
    "        if type(subjects) is str:\n",
    "            s = subjects.split(\",\")\n",
    "        else:\n",
    "            if type(subjects) is list:\n",
    "                s = subjects\n",
    "            else:\n",
    "                s = []\n",
    "        s = [t.lstrip().rstrip() for t in s]\n",
    "        tag_lists.append(s)\n",
    "        for tag in s:\n",
    "            tags.append(tag)\n",
    "    df['tags'] = tag_lists\n",
    "    return df, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b376ef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select claims from relevant categories\n",
    "health_tags = ['Health', 'Health News', \"Health Care\", 'Medical', 'Public Health']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a872b84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for masking dataframe with relevant tags\n",
    "def health(x):\n",
    "    for t in health_tags:\n",
    "        if t in x:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724fd1f0",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ae45172e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df (3596, 12)\n"
     ]
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "data_file_path = \"/Users/gwenythportillowightman/OneDrive - Johns Hopkins/fall-2022/interpretable_ml_design/PUBHEALTH/train.tsv\"          \n",
    "\n",
    "df = pd.read_csv(data_file_path, sep='\\t')\n",
    "df, df_tags = format_tags(df)\n",
    "\n",
    "mask = df['tags'].apply(lambda x: health(x))\n",
    "df = df[mask]\n",
    "\n",
    "# text_col contains the column name of where claims are found\n",
    "# answer_col contains the column name of where post labels (true, false, etc.) are found\n",
    "text_col = \"text\"\n",
    "answer_col = \"label\"\n",
    "\n",
    "# Rename the claim column to \"text\" and label column to \"label_categorical\"\n",
    "df.rename(columns = {\"claim\": \"text\", \"label\": \"label_categorical\"}, inplace = True)\n",
    "# Make the categorical labels into numbers (0, 1, 2, 3)\n",
    "df[\"label\"] = pd.factorize(df[\"label_categorical\"])[0]\n",
    "df = df.dropna(subset=[text_col])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Make a copy of the 'text' column\n",
    "df['text_original'] = df['text']\n",
    "\n",
    "print(f\"Shape of df {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5e170e",
   "metadata": {},
   "source": [
    "### Prepare to preprocess text claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c281cfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gwenythportillowightman/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "nltk.download('stopwords')\n",
    "s = stopwords.words('english')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    text = [ps.lemmatize(word) for word in text if not word in s]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603302cb",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c3350a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed claims\n",
      "0    britain reveal trial criterion coronavirus ant...\n",
      "1    u say result encouraging healthcare delivery r...\n",
      "2    latest trial j j talc litigation get way calif...\n",
      "3       democrat hoping flip house trash talking trump\n",
      "4        sex tech woman led startup pop ce gadget show\n",
      "5                             waxed apple cause cancer\n",
      "6    rhode island become second state mandate vacci...\n",
      "7    brazil city lurch lockdown amid virus crisis r...\n",
      "8    slovakia new government sharply ramp coronavir...\n",
      "9                       coronavirus simply common cold\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "ps = nltk.wordnet.WordNetLemmatizer()\n",
    "for i in range(df.shape[0]):\n",
    "    text = df.loc[i,'text']\n",
    "    text = preprocess_text(text)\n",
    "    df.loc[i, 'text'] = text\n",
    "    X_train = df['text']\n",
    "y_train = df['label']\n",
    "\n",
    "print(\"Preprocessed claims\")\n",
    "print(df['text'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa371b8",
   "metadata": {},
   "source": [
    "## KNN Model\n",
    "\n",
    "Returns the top k most similar sentences from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7453eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_Model():\n",
    "    def __init__(self, k=3, distance_type = 'path', preprocess=True):\n",
    "        self.k = k\n",
    "        self.distance_type = distance_type\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    # This function is used for training\n",
    "    def fit(self, x_train, y_train):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def split_input(self, input_sentence):\n",
    "        test_corpus = []\n",
    "        \n",
    "        # Preprocess the full x_test input\n",
    "        input_sentence_copy = copy.deepcopy(input_sentence)\n",
    "        if self.preprocess:\n",
    "            input_sentence_copy = preprocess_text(input_sentence_copy)\n",
    "        \n",
    "        # Preprocess sentences of the input\n",
    "        sentences = sent_tokenize(input_sentence)\n",
    "        for sentence in sentences:\n",
    "            if self.preprocess:\n",
    "                sentence = preprocess_text(sentence)\n",
    "            test_corpus.append(sentence)\n",
    "            \n",
    "        if len(test_corpus) > 1:\n",
    "            test_corpus.append(input_sentence_copy)\n",
    "        \n",
    "        return test_corpus\n",
    "\n",
    "    # Returns the k most similar sentences for the input sentence\n",
    "    # Predict returns the n similar sentences as a list of tuples [(sentence, score), (sentence, score), ...]\n",
    "    # Takes in only one input at a time\n",
    "    def predict(self, x_test):\n",
    "        test_corpus = self.split_input(x_test)\n",
    "            \n",
    "        self.x_test = test_corpus\n",
    "    \n",
    "        # {score: [(index of sentence in `test_corpus`, similar sentence index in `dataset`)], ...}\n",
    "        all_top_scores_dict = {}\n",
    "\n",
    "        # Iterate over sentences of the input\n",
    "        for i in range(len(self.x_test)):\n",
    "            print(f\"Getting similar sentences for \\\"{self.x_test[i]}\\\" ({i+1}/{len(self.x_test)})\")\n",
    "            \n",
    "            # {score: similar_sentence_index_in_`dataset`, ...}\n",
    "            score_to_index_dict = {}\n",
    "            \n",
    "            # Iterate over training examples and find sentence similarity scores\n",
    "            for j in range(self.x_train.shape[0]): \n",
    "                score = self.document_similarity(self.x_test[i], self.x_train[j])\n",
    "                score_to_index_dict[score] = j\n",
    "\n",
    "            sorted_scores = list(score_to_index_dict.keys())\n",
    "            sorted_scores.sort(reverse=True)\n",
    "\n",
    "            # Get the top k similar sentences for the current sentence (x_test[i])\n",
    "            for k in range(self.k):\n",
    "                score = sorted_scores[k]\n",
    "                \n",
    "                if score in all_top_scores_dict:\n",
    "                    all_top_scores_dict[score].append( (i, score_to_index_dict[score]) )\n",
    "                else:\n",
    "                    all_top_scores_dict[score] = [ (i, score_to_index_dict[score]) ]\n",
    "                    \n",
    "        # Get the top k scoring sentences and similar sentences from all_top_scores_dict\n",
    "        sorted_scores = list(all_top_scores_dict.keys())\n",
    "        sorted_scores.sort(reverse=True)\n",
    "        \n",
    "        # [ ((index_of_sentence_in_input, index_of_similar_sentence_in_`dataset`), score), ...]\n",
    "        similar_texts_list = []\n",
    "        \n",
    "        for k in range(self.k):\n",
    "            score = sorted_scores[k]\n",
    "            new_tuple = (all_top_scores_dict[score], score)\n",
    "            similar_texts_list.append(new_tuple)\n",
    "\n",
    "        return similar_texts_list\n",
    "    \n",
    "    def convert_tag(self, tag):\n",
    "        \"\"\"Convert the tag given by nltk.pos_tag to the tag used by wordnet.synsets\"\"\"\n",
    "        tag_dict = {'N': 'n', 'J': 'a', 'R': 'r', 'V': 'v'}\n",
    "        try:\n",
    "            return tag_dict[tag[0]]\n",
    "        except KeyError:\n",
    "            return None\n",
    "\n",
    "    def doc_to_synsets(self, doc):\n",
    "        \"\"\"\n",
    "            Returns a list of synsets in document.\n",
    "            Tokenizes and tags the words in the document doc.\n",
    "            Then finds the first synset for each word/tag combination.\n",
    "        If a synset is not found for that combination it is skipped.\n",
    "\n",
    "        Args:\n",
    "            doc: string to be converted\n",
    "\n",
    "        Returns:\n",
    "            list of synsets\n",
    "        \"\"\"\n",
    "        tokens = word_tokenize(doc+' ')\n",
    "        \n",
    "        l = []\n",
    "        tags = nltk.pos_tag([tokens[0] + ' ']) if len(tokens) == 1 else nltk.pos_tag(tokens)\n",
    "        \n",
    "        for token, tag in zip(tokens, tags):\n",
    "            syntag = self.convert_tag(tag[1])\n",
    "            syns = wn.synsets(token, syntag)\n",
    "            if (len(syns) > 0):\n",
    "                l.append(syns[0])\n",
    "        return l  \n",
    "    \n",
    "    def similarity_score(self, s1, s2, distance_type = 'path'):\n",
    "        \"\"\"\n",
    "        Calculate the normalized similarity score of s1 onto s2\n",
    "        For each synset in s1, finds the synset in s2 with the largest similarity value.\n",
    "        Sum of all of the largest similarity values and normalize this value by dividing it by the\n",
    "        number of largest similarity values found.\n",
    "\n",
    "        Args:\n",
    "          s1, s2: list of synsets from doc_to_synsets\n",
    "\n",
    "        Returns:\n",
    "          normalized similarity score of s1 onto s2\n",
    "        \"\"\"\n",
    "        s1_largest_scores = []\n",
    "\n",
    "        for i, s1_synset in enumerate(s1, 0):\n",
    "            max_score = 0\n",
    "            for s2_synset in s2:\n",
    "                if distance_type == 'path':\n",
    "                    score = s1_synset.path_similarity(s2_synset, simulate_root = False)\n",
    "                else:\n",
    "                    score = s1_synset.wup_similarity(s2_synset)                  \n",
    "                if score != None:\n",
    "                    if score > max_score:\n",
    "                        max_score = score\n",
    "\n",
    "            if max_score != 0:\n",
    "                s1_largest_scores.append(max_score)\n",
    "\n",
    "        mean_score = np.mean(s1_largest_scores)\n",
    "\n",
    "        return mean_score \n",
    "    \n",
    "    def document_similarity(self, doc1, doc2):\n",
    "        \"\"\"Finds the similarity between doc1 and doc2\"\"\"\n",
    "\n",
    "        synsets1 = self.doc_to_synsets(doc1)\n",
    "        synsets2 = self.doc_to_synsets(doc2)\n",
    "          \n",
    "        return (self.similarity_score(synsets1, synsets2) + self.similarity_score(synsets2, synsets1)) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad9d1e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting similar sentences for \"They are coated in toxic chemicals.\" (1/3)\n",
      "Getting similar sentences for \"Dryer sheets are one of the very worst things from a chemical allergy standpoint.\" (2/3)\n",
      "Getting similar sentences for \"They are coated in toxic chemicals. Dryer sheets are one of the very worst things from a chemical allergy standpoint.\" (3/3)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "k_value = 3\n",
    "preprocess = False\n",
    "\n",
    "classifier = KNN_Model(preprocess=preprocess, k=k_value, distance_type='path')\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "input_text = 'They are coated in toxic chemicals. Dryer sheets are one of the very worst things from a chemical allergy standpoint.'\n",
    "\n",
    "input_sentences = classifier.split_input(input_text)\n",
    "\n",
    "y_pred = classifier.predict(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d2c3d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "# print(f\"Top {k_value} similar examples:\")\n",
    "\n",
    "unique_similar_sentences = []\n",
    "all_similar_sentences = []\n",
    "similar_sentence_to_original_sentence_dict = {}  # Value is a tuple like (original_sentence, score)\n",
    "\n",
    "# Print out the k most similar sentences (across all sentences in input)\n",
    "for i, result in enumerate (y_pred):\n",
    "    original_sentence_index = result[0][0][0]\n",
    "    \n",
    "    if original_sentence_index == len(input_sentences):\n",
    "        original_sentence = input_text\n",
    "    else:\n",
    "        original_sentence = input_sentences[original_sentence_index]\n",
    "    \n",
    "    similar_sentence_index = result[0][0][1]\n",
    "    similar_sentence_data = df.iloc[[similar_sentence_index]].values.tolist()[0]\n",
    "    \n",
    "    text_original_column_index = 11\n",
    "    label_categorical_column_index = 7\n",
    "    \n",
    "    score = result[1]\n",
    "    \n",
    "    similar_sentence = similar_sentence_data[text_original_column_index]\n",
    "    all_similar_sentences.append(similar_sentence)\n",
    "    \n",
    "    original_sentence_score_tuple = (original_sentence, score)\n",
    "    if similar_sentence in similar_sentence_to_original_sentence_dict:\n",
    "        similar_sentence_to_original_sentence_dict[similar_sentence].append(original_sentence_score_tuple)\n",
    "    else:\n",
    "        similar_sentence_to_original_sentence_dict[similar_sentence] = [original_sentence_score_tuple]\n",
    "        \n",
    "    if similar_sentence in unique_similar_sentences:\n",
    "        continue\n",
    "    else:\n",
    "        unique_similar_sentences.append(similar_sentence)\n",
    "    \n",
    "#     print(f'Original sentence: {original_sentence}')\n",
    "#     print(f'Similar sentence: {similar_sentence}')\n",
    "#     print(f'Label: {similar_sentence_data[label_categorical_column_index]}')\n",
    "#     print(f'Score: {score}')\n",
    "#     print()\n",
    "    \n",
    "similar_sentence_counter = Counter(all_similar_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ba7372b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Most common similar sentences for input text ------\n",
      "\n",
      "SIMILAR SENTENCE: 'Dryer sheets permeated with fabric softener contain at least seven dangerous toxic chemicals.'; COUNT: 3\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] They are coated in toxic chemicals. Dryer sheets are one of the very worst things from a chemical allergy standpoint.  (0.6081337854065126)\n",
      "   - Dryer sheets are one of the very worst things from a chemical allergy standpoint.  (0.5304547882672882)\n",
      "   - They are coated in toxic chemicals.  (0.6266666666666667)\n",
      "\n",
      "SIMILAR SENTENCE: 'English Channel dolphins carry ‘toxic cocktail’ of chemicals.'; COUNT: 2\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] They are coated in toxic chemicals. Dryer sheets are one of the very worst things from a chemical allergy standpoint.  (0.44597649824922553)\n",
      "   - They are coated in toxic chemicals.  (0.622790404040404)\n",
      "\n",
      "SIMILAR SENTENCE: 'Ebola gives U.S. 'preppers' another reason to prepare for worst.'; COUNT: 2\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] They are coated in toxic chemicals. Dryer sheets are one of the very worst things from a chemical allergy standpoint.  (0.3699967330649149)\n",
      "   - Dryer sheets are one of the very worst things from a chemical allergy standpoint.  (0.3786071181904515)\n",
      "\n",
      "SIMILAR SENTENCE: 'States, military clash on cleanup of toxic chemicals.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - They are coated in toxic chemicals.  (0.4291229603729604)\n",
      "\n",
      "SIMILAR SENTENCE: 'Health study related to chemical at Pease will go ahead.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - They are coated in toxic chemicals.  (0.42808441558441557)\n",
      "\n",
      "SIMILAR SENTENCE: '\"Doctors are now warning parents to never use baby wipes because they contain a \"\"chemical\"\" called methylisothiazolinone.\"'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - They are coated in toxic chemicals.  (0.4068181818181818)\n",
      "\n",
      "SIMILAR SENTENCE: 'Study links chemical exposure to breast cancer.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - They are coated in toxic chemicals.  (0.4048433919022154)\n",
      "\n",
      "SIMILAR SENTENCE: 'In new headache, WeWork says it found cancer-causing chemical in its phone booths.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - They are coated in toxic chemicals.  (0.4029686000274235)\n",
      "\n",
      "SIMILAR SENTENCE: 'Pampers Dry Max diapers commonly cause severe diaper rash and chemical burns on infants.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - They are coated in toxic chemicals.  (0.3903457653457653)\n",
      "\n",
      "SIMILAR SENTENCE: 'A new analysis of the residue on common American foods showed high levels of the herbicide glyphosate and Monsanto, the EPA, and the FDA are in cahoots to silence word of the chemical’s harmful effects and its high concentration in foods.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - They are coated in toxic chemicals.  (0.389360754985755)\n",
      "\n",
      "SIMILAR SENTENCE: 'Philadelphia tops list of U.S. most toxic cities.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - They are coated in toxic chemicals.  (0.3533653846153846)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "most_common = similar_sentence_counter.most_common()\n",
    "\n",
    "print(\"------- Most common similar sentences for input text ------\")\n",
    "print()\n",
    "\n",
    "for (similar_sentence, count) in most_common:\n",
    "    print(f'SIMILAR SENTENCE: \\'{similar_sentence}\\'; COUNT: {count}')\n",
    "    original_sentence_score_tuple_list = similar_sentence_to_original_sentence_dict[similar_sentence]\n",
    "    \n",
    "    # Sort the tuples by the length of the first object in the tuple so that if the full input_text is \n",
    "    #    one of the similar sentences, it will be printed first\n",
    "    original_sentence_score_tuple_list.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    \n",
    "    print('ORIGINAL SENTENCES:')\n",
    "    for original_sentence_score_tuple in original_sentence_score_tuple_list:\n",
    "        original_sentence = original_sentence_score_tuple[0]\n",
    "        score = original_sentence_score_tuple[1]\n",
    "        if original_sentence == input_text:\n",
    "            print(f'   - [***FULL INPUT TEXT***] {original_sentence}  ({score})')\n",
    "        else:\n",
    "            print(f'   - {original_sentence}  ({score})')\n",
    "        \n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74f868d",
   "metadata": {},
   "source": [
    "## Use examples from PUBHEALTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "abcfbb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(data_file_path):\n",
    "    df = pd.read_csv(data_file_path, sep='\\t')\n",
    "    df, df_tags = format_tags(df)\n",
    "\n",
    "    mask = df['tags'].apply(lambda x: health(x))\n",
    "    df = df[mask]\n",
    "\n",
    "    # text_col contains the column name of where claims are found\n",
    "    # answer_col contains the column name of where post labels (true, false, etc.) are found\n",
    "    text_col = \"text\"\n",
    "    answer_col = \"label\"\n",
    "\n",
    "    # Rename the claim column to \"text\" and label column to \"label_categorical\"\n",
    "    df.rename(columns = {\"claim\": \"text\", \"label\": \"label_categorical\"}, inplace = True)\n",
    "    # Make the categorical labels into numbers (0, 1, 2, 3)\n",
    "    df[\"label\"] = pd.factorize(df[\"label_categorical\"])[0]\n",
    "    df = df.dropna(subset=[text_col])\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Make a copy of the 'text' column\n",
    "    df['text_original'] = df['text']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edec0100",
   "metadata": {},
   "source": [
    "### Functions to process an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059aa761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input_text(input_text, classifier):\n",
    "    input_sentences = classifier.split_input(input_text)\n",
    "\n",
    "    y_pred = classifier.predict(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff71dd6a",
   "metadata": {},
   "source": [
    "### 10 examples of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a87dcbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_file_path = \"/Users/gwenythportillowightman/OneDrive - Johns Hopkins/fall-2022/interpretable_ml_design/PUBHEALTH/test.tsv\"          \n",
    "\n",
    "test_df = prepare_df(test_data_file_path)\n",
    "\n",
    "test_data_subset = test_df[:10]\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    input_text = row['text']\n",
    "    process_input_text(input_text, classifier)\n",
    "    \n",
    "    unique_similar_sentences = []\n",
    "    all_similar_sentences = []\n",
    "    similar_sentence_to_original_sentence_dict = {}  # Value is a tuple like (original_sentence, score)\n",
    "\n",
    "    # Print out the k most similar sentences (across all sentences in input)\n",
    "    for i, result in enumerate (y_pred):\n",
    "        original_sentence_index = result[0][0][0]\n",
    "\n",
    "        if original_sentence_index == len(input_sentences):\n",
    "            original_sentence = input_text\n",
    "        else:\n",
    "            original_sentence = input_sentences[original_sentence_index]\n",
    "\n",
    "        similar_sentence_index = result[0][0][1]\n",
    "        similar_sentence_data = df.iloc[[similar_sentence_index]].values.tolist()[0]\n",
    "\n",
    "        text_original_column_index = 11\n",
    "        label_categorical_column_index = 7\n",
    "\n",
    "        score = result[1]\n",
    "\n",
    "        similar_sentence = similar_sentence_data[text_original_column_index]\n",
    "        all_similar_sentences.append(similar_sentence)\n",
    "\n",
    "        original_sentence_score_tuple = (original_sentence, score)\n",
    "        if similar_sentence in similar_sentence_to_original_sentence_dict:\n",
    "            similar_sentence_to_original_sentence_dict[similar_sentence].append(original_sentence_score_tuple)\n",
    "        else:\n",
    "            similar_sentence_to_original_sentence_dict[similar_sentence] = [original_sentence_score_tuple]\n",
    "\n",
    "        if similar_sentence in unique_similar_sentences:\n",
    "            continue\n",
    "        else:\n",
    "            unique_similar_sentences.append(similar_sentence)\n",
    "\n",
    "    similar_sentence_counter = Counter(all_similar_sentences)\n",
    "    \n",
    "    most_common = similar_sentence_counter.most_common()\n",
    "\n",
    "    print(\"------- Most common similar sentences for input text ------\")\n",
    "    print()\n",
    "\n",
    "    for (similar_sentence, count) in most_common:\n",
    "        print(f'SIMILAR SENTENCE: \\'{similar_sentence}\\'; COUNT: {count}')\n",
    "        original_sentence_score_tuple_list = similar_sentence_to_original_sentence_dict[similar_sentence]\n",
    "\n",
    "        # Sort the tuples by the length of the first object in the tuple so that if the full input_text is \n",
    "        #    one of the similar sentences, it will be printed first\n",
    "        original_sentence_score_tuple_list.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "\n",
    "        print('ORIGINAL SENTENCES:')\n",
    "        for original_sentence_score_tuple in original_sentence_score_tuple_list:\n",
    "            original_sentence = original_sentence_score_tuple[0]\n",
    "            score = original_sentence_score_tuple[1]\n",
    "            if original_sentence == input_text:\n",
    "                print(f'   - [***FULL INPUT TEXT***] {original_sentence}  ({score})')\n",
    "            else:\n",
    "                print(f'   - {original_sentence}  ({score})')\n",
    "\n",
    "        print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
