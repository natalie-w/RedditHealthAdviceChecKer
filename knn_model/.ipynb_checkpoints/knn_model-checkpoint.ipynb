{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4bbd4be",
   "metadata": {},
   "source": [
    "# Reddit HealthAdviceChecKer Bot Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c812088",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb34f8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package genesis to\n",
      "[nltk_data]     /Users/gwenythportillowightman/nltk_data...\n",
      "[nltk_data]   Package genesis is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gwenythportillowightman/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/gwenythportillowightman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/gwenythportillowightman/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import genesis\n",
    "nltk.download('genesis')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "genesis_ic = wn.ic(genesis, False, 0.0)\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667ae606",
   "metadata": {},
   "source": [
    "## Filtering the dataset to health claims only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1b29755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format tags columns in df\n",
    "\n",
    "def format_tags(df):\n",
    "    tags = []\n",
    "    tag_lists = []\n",
    "\n",
    "    for subjects in df.subjects:\n",
    "        if type(subjects) is str:\n",
    "            s = subjects.split(\",\")\n",
    "        else:\n",
    "            if type(subjects) is list:\n",
    "                s = subjects\n",
    "            else:\n",
    "                s = []\n",
    "        s = [t.lstrip().rstrip() for t in s]\n",
    "        tag_lists.append(s)\n",
    "        for tag in s:\n",
    "            tags.append(tag)\n",
    "    df['tags'] = tag_lists\n",
    "    return df, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b376ef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select claims from relevant categories\n",
    "health_tags = ['Health', 'Health News', \"Health Care\", 'Medical', 'Public Health', 'ADHD', 'Health / Medical', 'Medical Myths', 'diet']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a872b84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for masking dataframe with relevant tags\n",
    "def health(x):\n",
    "    for t in health_tags:\n",
    "        if t in x:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724fd1f0",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae45172e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9832, 9)\n",
      "Shape of df (3638, 12)\n"
     ]
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "data_file_path = \"/Users/gwenythportillowightman/OneDrive - Johns Hopkins/fall-2022/interpretable_ml_design/PUBHEALTH/train.tsv\"          \n",
    "df = pd.read_csv(data_file_path, sep='\\t')\n",
    "print(df.shape)\n",
    "\n",
    "df, df_tags = format_tags(df)\n",
    "\n",
    "mask = df['tags'].apply(lambda x: health(x))\n",
    "df = df[mask]\n",
    "\n",
    "# text_col contains the column name of where claims are found\n",
    "# answer_col contains the column name of where post labels (true, false, etc.) are found\n",
    "text_col = \"text\"\n",
    "answer_col = \"label\"\n",
    "\n",
    "# Rename the claim column to \"text\" and label column to \"label_categorical\"\n",
    "df.rename(columns = {\"claim\": \"text\", \"label\": \"label_categorical\"}, inplace = True)\n",
    "# Make the categorical labels into numbers (0, 1, 2, 3)\n",
    "df[\"label\"] = pd.factorize(df[\"label_categorical\"])[0]\n",
    "df = df.dropna(subset=[text_col])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Make a copy of the 'text' column\n",
    "df['text_original'] = df['text']\n",
    "\n",
    "print(f\"Shape of df {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5568329",
   "metadata": {},
   "outputs": [],
   "source": [
    "already_appended = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "308d1cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3660, 12)\n"
     ]
    }
   ],
   "source": [
    "# Append the extra training sentences\n",
    "if not already_appended:\n",
    "\n",
    "    extra_sentences_file_path = '/Users/gwenythportillowightman/OneDrive - Johns Hopkins/fall-2022/interpretable_ml_design/extra_sentences.csv'\n",
    "    extra_sentences_df = pd.read_csv(extra_sentences_file_path)\n",
    "\n",
    "    df = pd.concat([df, extra_sentences_df])\n",
    "    print(df.shape)\n",
    "    \n",
    "    already_appended = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5e170e",
   "metadata": {},
   "source": [
    "### Prepare to preprocess text claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c281cfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gwenythportillowightman/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "nltk.download('stopwords')\n",
    "s = stopwords.words('english')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    text = [ps.lemmatize(word) for word in text if not word in s]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603302cb",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3350a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed claims\n",
      "0    case imported fruit vegetable contaminated hiv...\n",
      "1    year old montana died lead poisoning eating ca...\n",
      "2     sure wash fruit vegetable country report hiv aid\n",
      "3    lead candle help reach daily nutritional healt...\n",
      "4    apple cider vinegar mixed water speed metaboli...\n",
      "5    apple cider vinegar great way lose weight drin...\n",
      "6    dryer sheet full toxic chemical keep away chil...\n",
      "7               using dryer sheet laundry give allergy\n",
      "8    spoonful safflower oil day keep pound away saf...\n",
      "9           trying lose weight used safflower oil cook\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "ps = nltk.wordnet.WordNetLemmatizer()\n",
    "for index, row in df.iterrows():\n",
    "    text = row['text']\n",
    "    text = preprocess_text(text)\n",
    "    df.loc[index, 'text'] = text\n",
    "    X_train = df['text']\n",
    "y_train = df['label']\n",
    "\n",
    "print(\"Preprocessed claims\")\n",
    "print(df['text'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa371b8",
   "metadata": {},
   "source": [
    "## KNN Model\n",
    "\n",
    "Returns the top k most similar sentences from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f7453eec",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-41-b5e9c3e92920>, line 54)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-41-b5e9c3e92920>\"\u001b[0;36m, line \u001b[0;32m54\u001b[0m\n\u001b[0;31m    print(f'WHYBHIOJKHIJOJBKHBIJOJKBH BKJI {self.x_train[j]}'')\u001b[0m\n\u001b[0m                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "class KNN_Model():\n",
    "    def __init__(self, k=3, distance_type = 'path', preprocess=True):\n",
    "        self.k = k\n",
    "        self.distance_type = distance_type\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    # This function is used for training\n",
    "    def fit(self, x_train, y_train):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def split_input(self, input_sentence):\n",
    "        test_corpus = []\n",
    "        \n",
    "        # Preprocess the full x_test input\n",
    "        input_sentence_copy = copy.deepcopy(input_sentence)\n",
    "        if self.preprocess:\n",
    "            input_sentence_copy = preprocess_text(input_sentence_copy)\n",
    "        \n",
    "        # Preprocess sentences of the input\n",
    "        sentences = sent_tokenize(input_sentence)\n",
    "        for sentence in sentences:\n",
    "            if self.preprocess:\n",
    "                sentence = preprocess_text(sentence)\n",
    "            test_corpus.append(sentence)\n",
    "            \n",
    "        if len(test_corpus) > 1:\n",
    "            test_corpus.append(input_sentence_copy)\n",
    "        \n",
    "        print(test_corpus)\n",
    "        \n",
    "        return test_corpus\n",
    "\n",
    "    # Returns the k most similar sentences for the input sentence\n",
    "    # Predict returns the n similar sentences as a list of tuples [(sentence, score), (sentence, score), ...]\n",
    "    # Takes in only one input at a time\n",
    "    def predict(self, x_test):\n",
    "        test_corpus = self.split_input(x_test)\n",
    "            \n",
    "        self.x_test = test_corpus\n",
    "    \n",
    "        # {score: [(index of sentence in `test_corpus`, similar sentence index in `dataset`)], ...}\n",
    "        all_top_scores_dict = {}\n",
    "\n",
    "        # Iterate over sentences of the input\n",
    "        for i in range(len(self.x_test)):\n",
    "            print(f\"------- Getting similar sentences for \\\"{self.x_test[i]}\\\" ({i+1}/{len(self.x_test)}) ------\")\n",
    "            \n",
    "            # {score: similar_sentence_index_in_`dataset`, ...}\n",
    "            score_to_index_dict = {}\n",
    "            \n",
    "            # Iterate over training examples and find sentence similarity scores\n",
    "            for j in range(self.x_train.shape[0]): \n",
    "                print(f'WHYBHIOJKHIJOJBKHBIJOJKBH BKJI {self.x_train[j]}')\n",
    "                new_sentence = self.x_train[j].tolist()[0]\n",
    "                print(f'i,j {self.x_test[i]} ,,, {new_sentence}')\n",
    "                score = self.document_similarity(self.x_test[i], new_sentence)\n",
    "                score_to_index_dict[score] = j\n",
    "\n",
    "            sorted_scores = list(score_to_index_dict.keys())\n",
    "            sorted_scores.sort(reverse=True)\n",
    "\n",
    "            # Get the top k similar sentences for the current sentence (x_test[i])\n",
    "            for k in range(self.k):\n",
    "                score = sorted_scores[k]\n",
    "                \n",
    "                if score in all_top_scores_dict:\n",
    "                    all_top_scores_dict[score].append( (i, score_to_index_dict[score]) )\n",
    "                else:\n",
    "                    all_top_scores_dict[score] = [ (i, score_to_index_dict[score]) ]\n",
    "                    \n",
    "        # Get the top k scoring sentences and similar sentences from all_top_scores_dict\n",
    "        sorted_scores = list(all_top_scores_dict.keys())\n",
    "        sorted_scores.sort(reverse=True)\n",
    "        \n",
    "        # [ ((index_of_sentence_in_input, index_of_similar_sentence_in_`dataset`), score), ...]\n",
    "        similar_texts_list = []\n",
    "        \n",
    "        for k in range(self.k):\n",
    "            score = sorted_scores[k]\n",
    "            new_tuple = (all_top_scores_dict[score], score)\n",
    "            similar_texts_list.append(new_tuple)\n",
    "\n",
    "        return similar_texts_list\n",
    "    \n",
    "    def convert_tag(self, tag):\n",
    "        \"\"\"Convert the tag given by nltk.pos_tag to the tag used by wordnet.synsets\"\"\"\n",
    "        tag_dict = {'N': 'n', 'J': 'a', 'R': 'r', 'V': 'v'}\n",
    "        try:\n",
    "            return tag_dict[tag[0]]\n",
    "        except KeyError:\n",
    "            return None\n",
    "\n",
    "    def doc_to_synsets(self, doc):\n",
    "        \"\"\"\n",
    "            Returns a list of synsets in document.\n",
    "            Tokenizes and tags the words in the document doc.\n",
    "            Then finds the first synset for each word/tag combination.\n",
    "        If a synset is not found for that combination it is skipped.\n",
    "\n",
    "        Args:\n",
    "            doc: string to be converted\n",
    "\n",
    "        Returns:\n",
    "            list of synsets\n",
    "        \"\"\"\n",
    "        print(f\"DOC DOC DOC {doc}\")\n",
    "        tokens = word_tokenize(doc+' ')\n",
    "        \n",
    "        l = []\n",
    "        tags = nltk.pos_tag([tokens[0] + ' ']) if len(tokens) == 1 else nltk.pos_tag(tokens)\n",
    "        \n",
    "        for token, tag in zip(tokens, tags):\n",
    "            syntag = self.convert_tag(tag[1])\n",
    "            syns = wn.synsets(token, syntag)\n",
    "            if (len(syns) > 0):\n",
    "                l.append(syns[0])\n",
    "        return l  \n",
    "    \n",
    "    def similarity_score(self, s1, s2, distance_type = 'path'):\n",
    "        \"\"\"\n",
    "        Calculate the normalized similarity score of s1 onto s2\n",
    "        For each synset in s1, finds the synset in s2 with the largest similarity value.\n",
    "        Sum of all of the largest similarity values and normalize this value by dividing it by the\n",
    "        number of largest similarity values found.\n",
    "\n",
    "        Args:\n",
    "          s1, s2: list of synsets from doc_to_synsets\n",
    "\n",
    "        Returns:\n",
    "          normalized similarity score of s1 onto s2\n",
    "        \"\"\"\n",
    "        s1_largest_scores = []\n",
    "\n",
    "        for i, s1_synset in enumerate(s1, 0):\n",
    "            max_score = 0\n",
    "            for s2_synset in s2:\n",
    "                if distance_type == 'path':\n",
    "                    score = s1_synset.path_similarity(s2_synset, simulate_root = False)\n",
    "                else:\n",
    "                    score = s1_synset.wup_similarity(s2_synset)                  \n",
    "                if score != None:\n",
    "                    if score > max_score:\n",
    "                        max_score = score\n",
    "\n",
    "            if max_score != 0:\n",
    "                s1_largest_scores.append(max_score)\n",
    "\n",
    "        mean_score = np.mean(s1_largest_scores)\n",
    "\n",
    "        return mean_score \n",
    "    \n",
    "    def document_similarity(self, doc1, doc2):\n",
    "        \"\"\"Finds the similarity between doc1 and doc2\"\"\"\n",
    "\n",
    "        synsets1 = self.doc_to_synsets(doc1)\n",
    "        synsets2 = self.doc_to_synsets(doc2)\n",
    "          \n",
    "        return (self.similarity_score(synsets1, synsets2) + self.similarity_score(synsets2, synsets1)) / 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ab681c",
   "metadata": {},
   "source": [
    "## Single example (dryer sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9d1e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_value = 3\n",
    "preprocess = False\n",
    "\n",
    "classifier = KNN_Model(preprocess=preprocess, k=k_value, distance_type='path')\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f51ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = 'They are coated in toxic chemicals. Dryer sheets are one of the very worst things from a chemical allergy standpoint.'\n",
    "\n",
    "input_sentences = classifier.split_input(input_text)\n",
    "\n",
    "y_pred = classifier.predict(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1dd4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "# print(f\"Top {k_value} similar examples:\")\n",
    "\n",
    "unique_similar_sentences = []\n",
    "all_similar_sentences = []\n",
    "similar_sentence_to_original_sentence_dict = {}  # Value is a tuple like (original_sentence, score)\n",
    "\n",
    "# Print out the k most similar sentences (across all sentences in input)\n",
    "for i, result in enumerate (y_pred):\n",
    "    original_sentence_index = result[0][0][0]\n",
    "    \n",
    "    if original_sentence_index == len(input_sentences):\n",
    "        original_sentence = input_text\n",
    "    else:\n",
    "        original_sentence = input_sentences[original_sentence_index]\n",
    "    \n",
    "    similar_sentence_index = result[0][0][1]\n",
    "    similar_sentence_data = df.iloc[[similar_sentence_index]].values.tolist()[0]\n",
    "    \n",
    "    text_original_column_index = 11\n",
    "    label_categorical_column_index = 7\n",
    "    \n",
    "    score = result[1]\n",
    "    \n",
    "    similar_sentence = similar_sentence_data[text_original_column_index]\n",
    "    all_similar_sentences.append(similar_sentence)\n",
    "    \n",
    "    original_sentence_score_tuple = (original_sentence, score)\n",
    "    if similar_sentence in similar_sentence_to_original_sentence_dict:\n",
    "        similar_sentence_to_original_sentence_dict[similar_sentence].append(original_sentence_score_tuple)\n",
    "    else:\n",
    "        similar_sentence_to_original_sentence_dict[similar_sentence] = [original_sentence_score_tuple]\n",
    "        \n",
    "    if similar_sentence in unique_similar_sentences:\n",
    "        continue\n",
    "    else:\n",
    "        unique_similar_sentences.append(similar_sentence)\n",
    "    \n",
    "#     print(f'Original sentence: {original_sentence}')\n",
    "#     print(f'Similar sentence: {similar_sentence}')\n",
    "#     print(f'Label: {similar_sentence_data[label_categorical_column_index]}')\n",
    "#     print(f'Score: {score}')\n",
    "#     print()\n",
    "    \n",
    "similar_sentence_counter = Counter(all_similar_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5115c220",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common = similar_sentence_counter.most_common()\n",
    "\n",
    "print(\"Most common similar sentences for input text\")\n",
    "print()\n",
    "\n",
    "for (similar_sentence, count) in most_common:\n",
    "    print(f'SIMILAR SENTENCE: \\'{similar_sentence}\\'; COUNT: {count}')\n",
    "    original_sentence_score_tuple_list = similar_sentence_to_original_sentence_dict[similar_sentence]\n",
    "    \n",
    "    # Sort the tuples by the length of the first object in the tuple so that if the full input_text is \n",
    "    #    one of the similar sentences, it will be printed first\n",
    "    original_sentence_score_tuple_list.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    \n",
    "    print('ORIGINAL SENTENCES:')\n",
    "    for original_sentence_score_tuple in original_sentence_score_tuple_list:\n",
    "        original_sentence = original_sentence_score_tuple[0]\n",
    "        score = original_sentence_score_tuple[1]\n",
    "        if original_sentence == input_text:\n",
    "            print(f'   - [***FULL INPUT TEXT***] {original_sentence}  ({score})')\n",
    "        else:\n",
    "            print(f'   - {original_sentence}  ({score})')\n",
    "        \n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4ccc18",
   "metadata": {},
   "source": [
    "## Use examples from PUBHEALTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e3d2f4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(data_file_path):\n",
    "    if data_file_path.endswith('tsv'):\n",
    "        df = pd.read_csv(data_file_path, sep='\\t')\n",
    "    else:  # assume csv\n",
    "        df = pd.read_csv(data_file_path)\n",
    "\n",
    "    df, df_tags = format_tags(df)\n",
    "\n",
    "    mask = df['tags'].apply(lambda x: health(x))\n",
    "    df = df[mask]\n",
    "\n",
    "    # text_col contains the column name of where claims are found\n",
    "    # answer_col contains the column name of where post labels (true, false, etc.) are found\n",
    "    text_col = \"text\"\n",
    "    answer_col = \"label\"\n",
    "\n",
    "    # Rename the claim column to \"text\" and label column to \"label_categorical\"\n",
    "    df.rename(columns = {\"claim\": \"text\", \"label\": \"label_categorical\"}, inplace = True)\n",
    "    # Make the categorical labels into numbers (0, 1, 2, 3)\n",
    "    df[\"label\"] = pd.factorize(df[\"label_categorical\"])[0]\n",
    "    df = df.dropna(subset=[text_col])\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Make a copy of the 'text' column\n",
    "    df['text_original'] = df['text']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adebc6df",
   "metadata": {},
   "source": [
    "### Functions to process an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad6809b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input_text(input_text, classifier):\n",
    "    input_sentences = classifier.split_input(input_text)\n",
    "\n",
    "    y_pred = classifier.predict(input_text)\n",
    "    \n",
    "    unique_similar_sentences = []\n",
    "    all_similar_sentences = []\n",
    "    similar_sentence_to_original_sentence_dict = {}  # Value is a tuple like (original_sentence, score)\n",
    "\n",
    "    # Print out the k most similar sentences (across all sentences in input)\n",
    "    for i, result in enumerate (y_pred):\n",
    "        original_sentence_index = result[0][0][0]\n",
    "\n",
    "        if original_sentence_index == len(input_sentences):\n",
    "            original_sentence = input_text\n",
    "        else:\n",
    "            original_sentence = input_sentences[original_sentence_index]\n",
    "\n",
    "        similar_sentence_index = result[0][0][1]\n",
    "        similar_sentence_data = df.iloc[[similar_sentence_index]].values.tolist()[0]\n",
    "\n",
    "        text_original_column_index = 11\n",
    "        label_categorical_column_index = 7\n",
    "#         label_column_index = \n",
    "\n",
    "        score = result[1]\n",
    "\n",
    "        similar_sentence = similar_sentence_data[text_original_column_index]\n",
    "        all_similar_sentences.append(similar_sentence)\n",
    "\n",
    "        original_sentence_score_tuple = (original_sentence, score)\n",
    "        if similar_sentence in similar_sentence_to_original_sentence_dict:\n",
    "            similar_sentence_to_original_sentence_dict[similar_sentence].append(original_sentence_score_tuple)\n",
    "        else:\n",
    "            similar_sentence_to_original_sentence_dict[similar_sentence] = [original_sentence_score_tuple]\n",
    "\n",
    "        if similar_sentence in unique_similar_sentences:\n",
    "            continue\n",
    "        else:\n",
    "            unique_similar_sentences.append(similar_sentence)\n",
    "\n",
    "    similar_sentence_counter = Counter(all_similar_sentences)\n",
    "    \n",
    "    most_common = similar_sentence_counter.most_common()\n",
    "\n",
    "    print(\" ~~~ Most common similar sentences for input text ~~~\")\n",
    "    print()\n",
    "\n",
    "    for (similar_sentence, count) in most_common:\n",
    "        print(f'SIMILAR SENTENCE: \\'{similar_sentence}\\'; COUNT: {count}')\n",
    "        original_sentence_score_tuple_list = similar_sentence_to_original_sentence_dict[similar_sentence]\n",
    "\n",
    "        # Sort the tuples by the length of the first object in the tuple so that if the full input_text is \n",
    "        #    one of the similar sentences, it will be printed first\n",
    "        original_sentence_score_tuple_list.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073245a7",
   "metadata": {},
   "source": [
    "### 10 examples of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e599c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_file_path = \"/Users/gwenythportillowightman/OneDrive - Johns Hopkins/fall-2022/interpretable_ml_design/PUBHEALTH/test.tsv\"          \n",
    "\n",
    "test_df = prepare_df(test_data_file_path)\n",
    "\n",
    "test_data_subset = test_df[:10]\n",
    "\n",
    "for index, row in test_df_subset.iterrows():\n",
    "    input_text = row['text']\n",
    "    process_input_text(input_text, classifier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae652c8",
   "metadata": {},
   "source": [
    "### Examples from [User Study Examples](https://docs.google.com/spreadsheets/d/1BF-PR27TVwq9P6pcZQ9eHCOuQVP9nT989nmta5CbcGM/edit#gid=63935314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2353e42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier for user study examples\n",
    "k_value = 5\n",
    "preprocess = False\n",
    "\n",
    "classifier = KNN_Model(preprocess=preprocess, k=k_value, distance_type='path')\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f3fba366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You can get HIV from fruits or vegetables from other countries.']\n",
      "['You can get HIV from fruits or vegetables from other countries.']\n",
      "------- Getting similar sentences for \"You can get HIV from fruits or vegetables from other countries.\" (1/1) ------\n",
      "i,j You can get HIV from fruits or vegetables from other countries. ,,, case imported fruit vegetable contaminated hiv positive blood\n",
      "DOC DOC DOC You can get HIV from fruits or vegetables from other countries.\n",
      "DOC DOC DOC case imported fruit vegetable contaminated hiv positive blood\n",
      "i,j You can get HIV from fruits or vegetables from other countries. ,,, year old montana died lead poisoning eating candy cane shaped candle\n",
      "DOC DOC DOC You can get HIV from fruits or vegetables from other countries.\n",
      "DOC DOC DOC year old montana died lead poisoning eating candy cane shaped candle\n",
      "i,j You can get HIV from fruits or vegetables from other countries. ,,, sure wash fruit vegetable country report hiv aid\n",
      "DOC DOC DOC You can get HIV from fruits or vegetables from other countries.\n",
      "DOC DOC DOC sure wash fruit vegetable country report hiv aid\n",
      "i,j You can get HIV from fruits or vegetables from other countries. ,,, lead candle help reach daily nutritional health goal\n",
      "DOC DOC DOC You can get HIV from fruits or vegetables from other countries.\n",
      "DOC DOC DOC lead candle help reach daily nutritional health goal\n",
      "i,j You can get HIV from fruits or vegetables from other countries. ,,, apple cider vinegar mixed water speed metabolism help keep weight\n",
      "DOC DOC DOC You can get HIV from fruits or vegetables from other countries.\n",
      "DOC DOC DOC apple cider vinegar mixed water speed metabolism help keep weight\n",
      "i,j You can get HIV from fruits or vegetables from other countries. ,,, apple cider vinegar great way lose weight drink one tablespoon every morning\n",
      "DOC DOC DOC You can get HIV from fruits or vegetables from other countries.\n",
      "DOC DOC DOC apple cider vinegar great way lose weight drink one tablespoon every morning\n",
      "i,j You can get HIV from fruits or vegetables from other countries. ,,, dryer sheet full toxic chemical keep away child care\n",
      "DOC DOC DOC You can get HIV from fruits or vegetables from other countries.\n",
      "DOC DOC DOC dryer sheet full toxic chemical keep away child care\n",
      "i,j You can get HIV from fruits or vegetables from other countries. ,,, using dryer sheet laundry give allergy\n",
      "DOC DOC DOC You can get HIV from fruits or vegetables from other countries.\n",
      "DOC DOC DOC using dryer sheet laundry give allergy\n",
      "i,j You can get HIV from fruits or vegetables from other countries. ,,, spoonful safflower oil day keep pound away safflower oil supplement great way lose weight\n",
      "DOC DOC DOC You can get HIV from fruits or vegetables from other countries.\n",
      "DOC DOC DOC spoonful safflower oil day keep pound away safflower oil supplement great way lose weight\n",
      "i,j You can get HIV from fruits or vegetables from other countries. ,,, trying lose weight used safflower oil cook\n",
      "DOC DOC DOC You can get HIV from fruits or vegetables from other countries.\n",
      "DOC DOC DOC trying lose weight used safflower oil cook\n",
      "i,j You can get HIV from fruits or vegetables from other countries. ,,, delayed release adhd medication provides day symptom control\n",
      "DOC DOC DOC You can get HIV from fruits or vegetables from other countries.\n",
      "DOC DOC DOC delayed release adhd medication provides day symptom control\n",
      "i,j You can get HIV from fruits or vegetables from other countries. ,,, extended release adderall reach maximum blood level within hour taking effect last hour\n",
      "DOC DOC DOC You can get HIV from fruits or vegetables from other countries.\n",
      "DOC DOC DOC extended release adderall reach maximum blood level within hour taking effect last hour\n",
      "i,j You can get HIV from fruits or vegetables from other countries. ,,, vaping cause lung problem actually improves breathing\n",
      "DOC DOC DOC You can get HIV from fruits or vegetables from other countries.\n",
      "DOC DOC DOC vaping cause lung problem actually improves breathing\n",
      "i,j You can get HIV from fruits or vegetables from other countries. ,,, cannot die due e cigarette use\n",
      "DOC DOC DOC You can get HIV from fruits or vegetables from other countries.\n",
      "DOC DOC DOC cannot die due e cigarette use\n",
      "i,j You can get HIV from fruits or vegetables from other countries. ,,, grandmother drank cold water ate died hour later\n",
      "DOC DOC DOC You can get HIV from fruits or vegetables from other countries.\n",
      "DOC DOC DOC grandmother drank cold water ate died hour later\n",
      "i,j You can get HIV from fruits or vegetables from other countries. ,,, cold water slows digestion drink eat\n",
      "DOC DOC DOC You can get HIV from fruits or vegetables from other countries.\n",
      "DOC DOC DOC cold water slows digestion drink eat\n",
      "i,j You can get HIV from fruits or vegetables from other countries. ,,, sunscreen actually make wrinkled cause skin cancer\n",
      "DOC DOC DOC You can get HIV from fruits or vegetables from other countries.\n",
      "DOC DOC DOC sunscreen actually make wrinkled cause skin cancer\n",
      "i,j You can get HIV from fruits or vegetables from other countries. ,,, sun exposure actually make resistant sun help fight skin cancer like vaccine\n",
      "DOC DOC DOC You can get HIV from fruits or vegetables from other countries.\n",
      "DOC DOC DOC sun exposure actually make resistant sun help fight skin cancer like vaccine\n",
      "i,j You can get HIV from fruits or vegetables from other countries. ,,, many people report using marijuana cope anxiety especially social anxiety disorder\n",
      "DOC DOC DOC You can get HIV from fruits or vegetables from other countries.\n",
      "DOC DOC DOC many people report using marijuana cope anxiety especially social anxiety disorder\n",
      "i,j You can get HIV from fruits or vegetables from other countries. ,,, study conducted specifically social anxiety disorder scarce condition fall umbrella qualifying condition many state medical marijuana program\n",
      "DOC DOC DOC You can get HIV from fruits or vegetables from other countries.\n",
      "DOC DOC DOC study conducted specifically social anxiety disorder scarce condition fall umbrella qualifying condition many state medical marijuana program\n",
      "i,j You can get HIV from fruits or vegetables from other countries. ,,, algae healthy vegetable find online\n",
      "DOC DOC DOC You can get HIV from fruits or vegetables from other countries.\n",
      "DOC DOC DOC algae healthy vegetable find online\n",
      "i,j You can get HIV from fruits or vegetables from other countries. ,,, drinking water river stream always safe water moving\n",
      "DOC DOC DOC You can get HIV from fruits or vegetables from other countries.\n",
      "DOC DOC DOC drinking water river stream always safe water moving\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'tolist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-906dc6c281ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mUSE_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Claims'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mprocess_input_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-e1301156ca54>\u001b[0m in \u001b[0;36mprocess_input_text\u001b[0;34m(input_text, classifier)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0minput_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0munique_similar_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-a3831e0384e8>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x_test)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;31m# Iterate over training examples and find sentence similarity scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0mnew_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'i,j {self.x_test[i]} ,,, {new_sentence}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'tolist'"
     ]
    }
   ],
   "source": [
    "user_study_examples_file_path = '/Users/gwenythportillowightman/OneDrive - Johns Hopkins/fall-2022/interpretable_ml_design/user_study_examples.csv'\n",
    "\n",
    "USE_df = pd.read_csv(user_study_examples_file_path)\n",
    "\n",
    "for index, row in USE_df.iterrows():\n",
    "    input_text = row['Claims']\n",
    "    process_input_text(input_text, classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
