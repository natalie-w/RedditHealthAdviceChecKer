{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4bbd4be",
   "metadata": {},
   "source": [
    "# Reddit HealthAdviceChecKer Bot Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c812088",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bb34f8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package genesis to\n",
      "[nltk_data]     /Users/gwenythportillowightman/nltk_data...\n",
      "[nltk_data]   Package genesis is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gwenythportillowightman/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/gwenythportillowightman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/gwenythportillowightman/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import genesis\n",
    "nltk.download('genesis')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "genesis_ic = wn.ic(genesis, False, 0.0)\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667ae606",
   "metadata": {},
   "source": [
    "## Filtering the dataset to health claims only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d1b29755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format tags columns in df\n",
    "\n",
    "def format_tags(df):\n",
    "    tags = []\n",
    "    tag_lists = []\n",
    "\n",
    "    for subjects in df.subjects:\n",
    "        if type(subjects) is str:\n",
    "            s = subjects.split(\",\")\n",
    "        else:\n",
    "            if type(subjects) is list:\n",
    "                s = subjects\n",
    "            else:\n",
    "                s = []\n",
    "        s = [t.lstrip().rstrip() for t in s]\n",
    "        tag_lists.append(s)\n",
    "        for tag in s:\n",
    "            tags.append(tag)\n",
    "    df['tags'] = tag_lists\n",
    "    return df, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b376ef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select claims from relevant categories\n",
    "health_tags = ['Health', 'Health News', \"Health Care\", 'Medical', 'Public Health']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a872b84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for masking dataframe with relevant tags\n",
    "def health(x):\n",
    "    for t in health_tags:\n",
    "        if t in x:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724fd1f0",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ae45172e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df (3596, 12)\n"
     ]
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "data_file_path = \"/Users/gwenythportillowightman/OneDrive - Johns Hopkins/fall-2022/interpretable_ml_design/PUBHEALTH/train.tsv\"          \n",
    "\n",
    "df = pd.read_csv(data_file_path, sep='\\t')\n",
    "df, df_tags = format_tags(df)\n",
    "\n",
    "mask = df['tags'].apply(lambda x: health(x))\n",
    "df = df[mask]\n",
    "\n",
    "# text_col contains the column name of where claims are found\n",
    "# answer_col contains the column name of where post labels (true, false, etc.) are found\n",
    "text_col = \"text\"\n",
    "answer_col = \"label\"\n",
    "\n",
    "# Rename the claim column to \"text\" and label column to \"label_categorical\"\n",
    "df.rename(columns = {\"claim\": \"text\", \"label\": \"label_categorical\"}, inplace = True)\n",
    "# Make the categorical labels into numbers (0, 1, 2, 3)\n",
    "df[\"label\"] = pd.factorize(df[\"label_categorical\"])[0]\n",
    "df = df.dropna(subset=[text_col])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Make a copy of the 'text' column\n",
    "df['text_original'] = df['text']\n",
    "\n",
    "print(f\"Shape of df {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5e170e",
   "metadata": {},
   "source": [
    "### Prepare to preprocess text claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c281cfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gwenythportillowightman/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "nltk.download('stopwords')\n",
    "s = stopwords.words('english')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    text = [ps.lemmatize(word) for word in text if not word in s]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603302cb",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c3350a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed claims\n",
      "0    britain reveal trial criterion coronavirus ant...\n",
      "1    u say result encouraging healthcare delivery r...\n",
      "2    latest trial j j talc litigation get way calif...\n",
      "3       democrat hoping flip house trash talking trump\n",
      "4        sex tech woman led startup pop ce gadget show\n",
      "5                             waxed apple cause cancer\n",
      "6    rhode island become second state mandate vacci...\n",
      "7    brazil city lurch lockdown amid virus crisis r...\n",
      "8    slovakia new government sharply ramp coronavir...\n",
      "9                       coronavirus simply common cold\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "ps = nltk.wordnet.WordNetLemmatizer()\n",
    "for i in range(df.shape[0]):\n",
    "    text = df.loc[i,'text']\n",
    "    text = preprocess_text(text)\n",
    "    df.loc[i, 'text'] = text\n",
    "    X_train = df['text']\n",
    "y_train = df['label']\n",
    "\n",
    "print(\"Preprocessed claims\")\n",
    "print(df['text'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa371b8",
   "metadata": {},
   "source": [
    "## KNN Model\n",
    "\n",
    "Returns the top k most similar sentences from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7453eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_Model():\n",
    "    def __init__(self, k=3, distance_type = 'path', preprocess=True):\n",
    "        self.k = k\n",
    "        self.distance_type = distance_type\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    # This function is used for training\n",
    "    def fit(self, x_train, y_train):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def split_input(self, input_sentence):\n",
    "        test_corpus = []\n",
    "        \n",
    "        # Preprocess the full x_test input\n",
    "        input_sentence_copy = copy.deepcopy(input_sentence)\n",
    "        if self.preprocess:\n",
    "            input_sentence_copy = preprocess_text(input_sentence_copy)\n",
    "        \n",
    "        # Preprocess sentences of the input\n",
    "        sentences = sent_tokenize(input_sentence)\n",
    "        for sentence in sentences:\n",
    "            if self.preprocess:\n",
    "                sentence = preprocess_text(sentence)\n",
    "            test_corpus.append(sentence)\n",
    "            \n",
    "        if len(test_corpus) > 1:\n",
    "            test_corpus.append(input_sentence_copy)\n",
    "        \n",
    "        return test_corpus\n",
    "\n",
    "    # Returns the k most similar sentences for the input sentence\n",
    "    # Predict returns the n similar sentences as a list of tuples [(sentence, score), (sentence, score), ...]\n",
    "    # Takes in only one input at a time\n",
    "    def predict(self, x_test):\n",
    "        test_corpus = self.split_input(x_test)\n",
    "            \n",
    "        self.x_test = test_corpus\n",
    "    \n",
    "        # {score: [(index of sentence in `test_corpus`, similar sentence index in `dataset`)], ...}\n",
    "        all_top_scores_dict = {}\n",
    "\n",
    "        # Iterate over sentences of the input\n",
    "        for i in range(len(self.x_test)):\n",
    "            print(f\"Getting similar sentences for \\\"{self.x_test[i]}\\\" ({i+1}/{len(self.x_test)})\")\n",
    "            \n",
    "            # {score: similar_sentence_index_in_`dataset`, ...}\n",
    "            score_to_index_dict = {}\n",
    "            \n",
    "            # Iterate over training examples and find sentence similarity scores\n",
    "            for j in range(self.x_train.shape[0]): \n",
    "                score = self.document_similarity(self.x_test[i], self.x_train[j])\n",
    "                score_to_index_dict[score] = j\n",
    "\n",
    "            sorted_scores = list(score_to_index_dict.keys())\n",
    "            sorted_scores.sort(reverse=True)\n",
    "\n",
    "            # Get the top k similar sentences for the current sentence (x_test[i])\n",
    "            for k in range(self.k):\n",
    "                score = sorted_scores[k]\n",
    "                \n",
    "                if score in all_top_scores_dict:\n",
    "                    all_top_scores_dict[score].append( (i, score_to_index_dict[score]) )\n",
    "                else:\n",
    "                    all_top_scores_dict[score] = [ (i, score_to_index_dict[score]) ]\n",
    "                    \n",
    "        # Get the top k scoring sentences and similar sentences from all_top_scores_dict\n",
    "        sorted_scores = list(all_top_scores_dict.keys())\n",
    "        sorted_scores.sort(reverse=True)\n",
    "        \n",
    "        # [ ((index_of_sentence_in_input, index_of_similar_sentence_in_`dataset`), score), ...]\n",
    "        similar_texts_list = []\n",
    "        \n",
    "        for k in range(self.k):\n",
    "            score = sorted_scores[k]\n",
    "            new_tuple = (all_top_scores_dict[score], score)\n",
    "            similar_texts_list.append(new_tuple)\n",
    "\n",
    "        return similar_texts_list\n",
    "    \n",
    "    def convert_tag(self, tag):\n",
    "        \"\"\"Convert the tag given by nltk.pos_tag to the tag used by wordnet.synsets\"\"\"\n",
    "        tag_dict = {'N': 'n', 'J': 'a', 'R': 'r', 'V': 'v'}\n",
    "        try:\n",
    "            return tag_dict[tag[0]]\n",
    "        except KeyError:\n",
    "            return None\n",
    "\n",
    "    def doc_to_synsets(self, doc):\n",
    "        \"\"\"\n",
    "            Returns a list of synsets in document.\n",
    "            Tokenizes and tags the words in the document doc.\n",
    "            Then finds the first synset for each word/tag combination.\n",
    "        If a synset is not found for that combination it is skipped.\n",
    "\n",
    "        Args:\n",
    "            doc: string to be converted\n",
    "\n",
    "        Returns:\n",
    "            list of synsets\n",
    "        \"\"\"\n",
    "        tokens = word_tokenize(doc+' ')\n",
    "        \n",
    "        l = []\n",
    "        tags = nltk.pos_tag([tokens[0] + ' ']) if len(tokens) == 1 else nltk.pos_tag(tokens)\n",
    "        \n",
    "        for token, tag in zip(tokens, tags):\n",
    "            syntag = self.convert_tag(tag[1])\n",
    "            syns = wn.synsets(token, syntag)\n",
    "            if (len(syns) > 0):\n",
    "                l.append(syns[0])\n",
    "        return l  \n",
    "    \n",
    "    def similarity_score(self, s1, s2, distance_type = 'path'):\n",
    "        \"\"\"\n",
    "        Calculate the normalized similarity score of s1 onto s2\n",
    "        For each synset in s1, finds the synset in s2 with the largest similarity value.\n",
    "        Sum of all of the largest similarity values and normalize this value by dividing it by the\n",
    "        number of largest similarity values found.\n",
    "\n",
    "        Args:\n",
    "          s1, s2: list of synsets from doc_to_synsets\n",
    "\n",
    "        Returns:\n",
    "          normalized similarity score of s1 onto s2\n",
    "        \"\"\"\n",
    "        s1_largest_scores = []\n",
    "\n",
    "        for i, s1_synset in enumerate(s1, 0):\n",
    "            max_score = 0\n",
    "            for s2_synset in s2:\n",
    "                if distance_type == 'path':\n",
    "                    score = s1_synset.path_similarity(s2_synset, simulate_root = False)\n",
    "                else:\n",
    "                    score = s1_synset.wup_similarity(s2_synset)                  \n",
    "                if score != None:\n",
    "                    if score > max_score:\n",
    "                        max_score = score\n",
    "\n",
    "            if max_score != 0:\n",
    "                s1_largest_scores.append(max_score)\n",
    "\n",
    "        mean_score = np.mean(s1_largest_scores)\n",
    "\n",
    "        return mean_score \n",
    "    \n",
    "    def document_similarity(self, doc1, doc2):\n",
    "        \"\"\"Finds the similarity between doc1 and doc2\"\"\"\n",
    "\n",
    "        synsets1 = self.doc_to_synsets(doc1)\n",
    "        synsets2 = self.doc_to_synsets(doc2)\n",
    "          \n",
    "        return (self.similarity_score(synsets1, synsets2) + self.similarity_score(synsets2, synsets1)) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad9d1e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting similar sentences for \"They are coated in toxic chemicals.\" (1/3)\n",
      "Getting similar sentences for \"Dryer sheets are one of the very worst things from a chemical allergy standpoint.\" (2/3)\n",
      "Getting similar sentences for \"They are coated in toxic chemicals. Dryer sheets are one of the very worst things from a chemical allergy standpoint.\" (3/3)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "k_value = 3\n",
    "preprocess = False\n",
    "\n",
    "classifier = KNN_Model(preprocess=preprocess, k=k_value, distance_type='path')\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "input_text = 'They are coated in toxic chemicals. Dryer sheets are one of the very worst things from a chemical allergy standpoint.'\n",
    "\n",
    "input_sentences = classifier.split_input(input_text)\n",
    "\n",
    "y_pred = classifier.predict(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "92aa4d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "# print(f\"Top {k_value} similar examples:\")\n",
    "\n",
    "unique_similar_sentences = []\n",
    "all_similar_sentences = []\n",
    "similar_sentence_to_original_sentence_dict = {}  # Value is a tuple like (original_sentence, score)\n",
    "\n",
    "# Print out the k most similar sentences (across all sentences in input)\n",
    "for i, result in enumerate (y_pred):\n",
    "    original_sentence_index = result[0][0][0]\n",
    "    \n",
    "    if original_sentence_index == len(input_sentences):\n",
    "        original_sentence = input_text\n",
    "    else:\n",
    "        original_sentence = input_sentences[original_sentence_index]\n",
    "    \n",
    "    similar_sentence_index = result[0][0][1]\n",
    "    similar_sentence_data = df.iloc[[similar_sentence_index]].values.tolist()[0]\n",
    "    \n",
    "    text_original_column_index = 11\n",
    "    label_categorical_column_index = 7\n",
    "    \n",
    "    score = result[1]\n",
    "    \n",
    "    similar_sentence = similar_sentence_data[text_original_column_index]\n",
    "    all_similar_sentences.append(similar_sentence)\n",
    "    \n",
    "    original_sentence_score_tuple = (original_sentence, score)\n",
    "    if similar_sentence in similar_sentence_to_original_sentence_dict:\n",
    "        similar_sentence_to_original_sentence_dict[similar_sentence].append(original_sentence_score_tuple)\n",
    "    else:\n",
    "        similar_sentence_to_original_sentence_dict[similar_sentence] = [original_sentence_score_tuple]\n",
    "        \n",
    "    if similar_sentence in unique_similar_sentences:\n",
    "        continue\n",
    "    else:\n",
    "        unique_similar_sentences.append(similar_sentence)\n",
    "    \n",
    "#     print(f'Original sentence: {original_sentence}')\n",
    "#     print(f'Similar sentence: {similar_sentence}')\n",
    "#     print(f'Label: {similar_sentence_data[label_categorical_column_index]}')\n",
    "#     print(f'Score: {score}')\n",
    "#     print()\n",
    "    \n",
    "similar_sentence_counter = Counter(all_similar_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f878c386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Most common similar sentences for input text ------\n",
      "\n",
      "SIMILAR SENTENCE: 'Dryer sheets permeated with fabric softener contain at least seven dangerous toxic chemicals.'; COUNT: 3\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] They are coated in toxic chemicals. Dryer sheets are one of the very worst things from a chemical allergy standpoint.  (0.6081337854065126)\n",
      "   - Dryer sheets are one of the very worst things from a chemical allergy standpoint.  (0.5304547882672882)\n",
      "   - They are coated in toxic chemicals.  (0.6266666666666667)\n",
      "\n",
      "SIMILAR SENTENCE: 'English Channel dolphins carry ‘toxic cocktail’ of chemicals.'; COUNT: 2\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] They are coated in toxic chemicals. Dryer sheets are one of the very worst things from a chemical allergy standpoint.  (0.44597649824922553)\n",
      "   - They are coated in toxic chemicals.  (0.622790404040404)\n",
      "\n",
      "SIMILAR SENTENCE: 'Ebola gives U.S. 'preppers' another reason to prepare for worst.'; COUNT: 2\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] They are coated in toxic chemicals. Dryer sheets are one of the very worst things from a chemical allergy standpoint.  (0.3699967330649149)\n",
      "   - Dryer sheets are one of the very worst things from a chemical allergy standpoint.  (0.3786071181904515)\n",
      "\n",
      "SIMILAR SENTENCE: 'States, military clash on cleanup of toxic chemicals.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - They are coated in toxic chemicals.  (0.4291229603729604)\n",
      "\n",
      "SIMILAR SENTENCE: 'Health study related to chemical at Pease will go ahead.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - They are coated in toxic chemicals.  (0.42808441558441557)\n",
      "\n",
      "SIMILAR SENTENCE: '\"Doctors are now warning parents to never use baby wipes because they contain a \"\"chemical\"\" called methylisothiazolinone.\"'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - They are coated in toxic chemicals.  (0.4068181818181818)\n",
      "\n",
      "SIMILAR SENTENCE: 'Study links chemical exposure to breast cancer.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - They are coated in toxic chemicals.  (0.4048433919022154)\n",
      "\n",
      "SIMILAR SENTENCE: 'In new headache, WeWork says it found cancer-causing chemical in its phone booths.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - They are coated in toxic chemicals.  (0.4029686000274235)\n",
      "\n",
      "SIMILAR SENTENCE: 'Pampers Dry Max diapers commonly cause severe diaper rash and chemical burns on infants.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - They are coated in toxic chemicals.  (0.3903457653457653)\n",
      "\n",
      "SIMILAR SENTENCE: 'A new analysis of the residue on common American foods showed high levels of the herbicide glyphosate and Monsanto, the EPA, and the FDA are in cahoots to silence word of the chemical’s harmful effects and its high concentration in foods.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - They are coated in toxic chemicals.  (0.389360754985755)\n",
      "\n",
      "SIMILAR SENTENCE: 'Philadelphia tops list of U.S. most toxic cities.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - They are coated in toxic chemicals.  (0.3533653846153846)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "most_common = similar_sentence_counter.most_common()\n",
    "\n",
    "print(\"------- Most common similar sentences for input text ------\")\n",
    "print()\n",
    "\n",
    "for (similar_sentence, count) in most_common:\n",
    "    print(f'SIMILAR SENTENCE: \\'{similar_sentence}\\'; COUNT: {count}')\n",
    "    original_sentence_score_tuple_list = similar_sentence_to_original_sentence_dict[similar_sentence]\n",
    "    \n",
    "    # Sort the tuples by the length of the first object in the tuple so that if the full input_text is \n",
    "    #    one of the similar sentences, it will be printed first\n",
    "    original_sentence_score_tuple_list.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    \n",
    "    print('ORIGINAL SENTENCES:')\n",
    "    for original_sentence_score_tuple in original_sentence_score_tuple_list:\n",
    "        original_sentence = original_sentence_score_tuple[0]\n",
    "        score = original_sentence_score_tuple[1]\n",
    "        if original_sentence == input_text:\n",
    "            print(f'   - [***FULL INPUT TEXT***] {original_sentence}  ({score})')\n",
    "        else:\n",
    "            print(f'   - {original_sentence}  ({score})')\n",
    "        \n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972807b8",
   "metadata": {},
   "source": [
    "## Use examples from PUBHEALTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3ba0e956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(data_file_path):\n",
    "    if data_file_path.endswith('tsv'):\n",
    "        df = pd.read_csv(data_file_path, sep='\\t')\n",
    "    else:  # assume csv\n",
    "        df = pd.read_csv(data_file_path)\n",
    "\n",
    "    df, df_tags = format_tags(df)\n",
    "\n",
    "    mask = df['tags'].apply(lambda x: health(x))\n",
    "    df = df[mask]\n",
    "\n",
    "    # text_col contains the column name of where claims are found\n",
    "    # answer_col contains the column name of where post labels (true, false, etc.) are found\n",
    "    text_col = \"text\"\n",
    "    answer_col = \"label\"\n",
    "\n",
    "    # Rename the claim column to \"text\" and label column to \"label_categorical\"\n",
    "    df.rename(columns = {\"claim\": \"text\", \"label\": \"label_categorical\"}, inplace = True)\n",
    "    # Make the categorical labels into numbers (0, 1, 2, 3)\n",
    "    df[\"label\"] = pd.factorize(df[\"label_categorical\"])[0]\n",
    "    df = df.dropna(subset=[text_col])\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Make a copy of the 'text' column\n",
    "    df['text_original'] = df['text']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d225dfc",
   "metadata": {},
   "source": [
    "### Functions to process an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f647d52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input_text(input_text, classifier):\n",
    "    input_sentences = classifier.split_input(input_text)\n",
    "\n",
    "    y_pred = classifier.predict(input_text)\n",
    "    \n",
    "    unique_similar_sentences = []\n",
    "    all_similar_sentences = []\n",
    "    similar_sentence_to_original_sentence_dict = {}  # Value is a tuple like (original_sentence, score)\n",
    "\n",
    "    # Print out the k most similar sentences (across all sentences in input)\n",
    "    for i, result in enumerate (y_pred):\n",
    "        original_sentence_index = result[0][0][0]\n",
    "\n",
    "        if original_sentence_index == len(input_sentences):\n",
    "            original_sentence = input_text\n",
    "        else:\n",
    "            original_sentence = input_sentences[original_sentence_index]\n",
    "\n",
    "        similar_sentence_index = result[0][0][1]\n",
    "        similar_sentence_data = df.iloc[[similar_sentence_index]].values.tolist()[0]\n",
    "\n",
    "        text_original_column_index = 11\n",
    "        label_categorical_column_index = 7\n",
    "#         label_column_index = \n",
    "\n",
    "        score = result[1]\n",
    "\n",
    "        similar_sentence = similar_sentence_data[text_original_column_index]\n",
    "        all_similar_sentences.append(similar_sentence)\n",
    "\n",
    "        original_sentence_score_tuple = (original_sentence, score)\n",
    "        if similar_sentence in similar_sentence_to_original_sentence_dict:\n",
    "            similar_sentence_to_original_sentence_dict[similar_sentence].append(original_sentence_score_tuple)\n",
    "        else:\n",
    "            similar_sentence_to_original_sentence_dict[similar_sentence] = [original_sentence_score_tuple]\n",
    "\n",
    "        if similar_sentence in unique_similar_sentences:\n",
    "            continue\n",
    "        else:\n",
    "            unique_similar_sentences.append(similar_sentence)\n",
    "\n",
    "    similar_sentence_counter = Counter(all_similar_sentences)\n",
    "    \n",
    "    most_common = similar_sentence_counter.most_common()\n",
    "\n",
    "    print(\"------- Most common similar sentences for input text ------\")\n",
    "    print()\n",
    "\n",
    "    for (similar_sentence, count) in most_common:\n",
    "        print(f'SIMILAR SENTENCE: \\'{similar_sentence}\\'; COUNT: {count}')\n",
    "        original_sentence_score_tuple_list = similar_sentence_to_original_sentence_dict[similar_sentence]\n",
    "\n",
    "        # Sort the tuples by the length of the first object in the tuple so that if the full input_text is \n",
    "        #    one of the similar sentences, it will be printed first\n",
    "        original_sentence_score_tuple_list.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "\n",
    "        print('ORIGINAL SENTENCES:')\n",
    "        for original_sentence_score_tuple in original_sentence_score_tuple_list:\n",
    "            original_sentence = original_sentence_score_tuple[0]\n",
    "            score = original_sentence_score_tuple[1]\n",
    "            if original_sentence == input_text:\n",
    "                print(f'   - [***FULL INPUT TEXT***] {original_sentence}  ({score})')\n",
    "            else:\n",
    "                print(f'   - {original_sentence}  ({score})')\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e48c059",
   "metadata": {},
   "source": [
    "### 10 examples of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "af064a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting similar sentences for \"Study says too many Americans still drink too much.\" (1/1)\n",
      "------- Most common similar sentences for input text ------\n",
      "\n",
      "SIMILAR SENTENCE: 'Study identifies best tests to predict Alzheimer's.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Study says too many Americans still drink too much.  (0.7666666666666666)\n",
      "\n",
      "SIMILAR SENTENCE: 'More U.S. households smoke-free, study says.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Study says too many Americans still drink too much.  (0.6041666666666667)\n",
      "\n",
      "SIMILAR SENTENCE: 'Common drugs hasten decline in elderly: study.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Study says too many Americans still drink too much.  (0.5833333333333334)\n",
      "\n",
      "Getting similar sentences for \"Roche's schizophrenia drug misses goal in two late-stage trials.\" (1/1)\n",
      "------- Most common similar sentences for input text ------\n",
      "\n",
      "SIMILAR SENTENCE: 'Synairgen gets green light for coronavirus drug trial.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Roche's schizophrenia drug misses goal in two late-stage trials.  (0.7023508898508898)\n",
      "\n",
      "SIMILAR SENTENCE: 'Amgen cholesterol drug meets goal of 3rd late stage trial.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Roche's schizophrenia drug misses goal in two late-stage trials.  (0.5595899470899471)\n",
      "\n",
      "SIMILAR SENTENCE: 'GSK's two-drug HIV Dovato treatment meets main goal in study.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Roche's schizophrenia drug misses goal in two late-stage trials.  (0.5356434240362812)\n",
      "\n",
      "Getting similar sentences for \"Doctors end protest to demand flu vaccines for migrants.\" (1/1)\n",
      "------- Most common similar sentences for input text ------\n",
      "\n",
      "SIMILAR SENTENCE: ''I thought I would never wake up,' Belgian doctor says after surviving COVID-19.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Doctors end protest to demand flu vaccines for migrants.  (0.6580528846153846)\n",
      "\n",
      "SIMILAR SENTENCE: 'Australia begins pre-clinical testing for coronavirus vaccine.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Doctors end protest to demand flu vaccines for migrants.  (0.656043956043956)\n",
      "\n",
      "SIMILAR SENTENCE: 'A viral photograph showing a canine coronavirus vaccine demonstrates that there should already exist a human vaccine for COVID-19.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Doctors end protest to demand flu vaccines for migrants.  (0.506043956043956)\n",
      "\n",
      "Getting similar sentences for \"The new supplement InteliGEN can boost brain function.\" (1/1)\n",
      "------- Most common similar sentences for input text ------\n",
      "\n",
      "SIMILAR SENTENCE: 'More drugmakers hike U.S. prices as new year begins.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] The new supplement InteliGEN can boost brain function.  (0.430715811965812)\n",
      "\n",
      "SIMILAR SENTENCE: 'Could the new coronavirus weaken 'anti-vaxxers'?.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] The new supplement InteliGEN can boost brain function.  (0.4190909090909091)\n",
      "\n",
      "SIMILAR SENTENCE: 'Slovakia's new government to sharply ramp up coronavirus testing.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] The new supplement InteliGEN can boost brain function.  (0.4122655122655123)\n",
      "\n",
      "Getting similar sentences for \"Jury finding upends Bayer's Roundup defense strategy: experts.\" (1/1)\n",
      "------- Most common similar sentences for input text ------\n",
      "\n",
      "SIMILAR SENTENCE: 'Bayer bets on 'silver bullet' defense in Roundup litigation; experts see hurdles.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Jury finding upends Bayer's Roundup defense strategy: experts.  (0.65995670995671)\n",
      "\n",
      "SIMILAR SENTENCE: 'U.S. judge appoints Ken Feinberg mediator for Bayer Roundup settlement talks.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Jury finding upends Bayer's Roundup defense strategy: experts.  (0.4708333333333333)\n",
      "\n",
      "SIMILAR SENTENCE: 'Bayer mediator dismisses report of $8 billion Roundup settlement.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Jury finding upends Bayer's Roundup defense strategy: experts.  (0.4534090909090909)\n",
      "\n",
      "Getting similar sentences for \"Lawmakers consider spending, saving $128.5 million surplus.\" (1/1)\n",
      "------- Most common similar sentences for input text ------\n",
      "\n",
      "SIMILAR SENTENCE: 'Connecticut lawmakers considering expanding seatbelt law.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Lawmakers consider spending, saving $128.5 million surplus.  (0.45344988344988346)\n",
      "\n",
      "SIMILAR SENTENCE: 'UK lawmakers criticize govt's stockpiling of Roche drug Tamiflu.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Lawmakers consider spending, saving $128.5 million surplus.  (0.44562728937728935)\n",
      "\n",
      "SIMILAR SENTENCE: '\"The ocean is now \"\"much more acidic . . . than it has been for many millions of years.\"'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Lawmakers consider spending, saving $128.5 million surplus.  (0.43417346542346547)\n",
      "\n",
      "Getting similar sentences for \"Public warned of possible measles exposure in Pennsylvania.\" (1/1)\n",
      "------- Most common similar sentences for input text ------\n",
      "\n",
      "SIMILAR SENTENCE: 'Measles outbreak that sickened 312 in Rockland declared over.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Public warned of possible measles exposure in Pennsylvania.  (0.6237611408199644)\n",
      "\n",
      "SIMILAR SENTENCE: 'Virginia health officials warn of possible measles exposure.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Public warned of possible measles exposure in Pennsylvania.  (0.5869107744107744)\n",
      "\n",
      "SIMILAR SENTENCE: 'Texas officials investigating possible tuberculosis exposure.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Public warned of possible measles exposure in Pennsylvania.  (0.5446127946127945)\n",
      "\n",
      "Getting similar sentences for \"Britain backs GSK's gene therapy for 'bubble boy' syndrome.\" (1/1)\n",
      "------- Most common similar sentences for input text ------\n",
      "\n",
      "SIMILAR SENTENCE: 'Some plateauing of London coronavirus outbreak, UK says.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Britain backs GSK's gene therapy for 'bubble boy' syndrome.  (0.623837818528221)\n",
      "\n",
      "SIMILAR SENTENCE: 'Wrestling Wonsettlers are now getting physical with therapy.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Britain backs GSK's gene therapy for 'bubble boy' syndrome.  (0.62375)\n",
      "\n",
      "SIMILAR SENTENCE: 'Experimental gene therapy improves sight in patients going blind.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Britain backs GSK's gene therapy for 'bubble boy' syndrome.  (0.5266506410256411)\n",
      "\n",
      "Getting similar sentences for \"South Korea court strikes down abortion law in landmark ruling.\" (1/1)\n",
      "------- Most common similar sentences for input text ------\n",
      "\n",
      "SIMILAR SENTENCE: 'Supreme Court leaves Kentucky’s ultrasound law in place.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] South Korea court strikes down abortion law in landmark ruling.  (0.47351190476190474)\n",
      "\n",
      "SIMILAR SENTENCE: 'U.S. appeals court allows Texas abortion curbs amid pandemic.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] South Korea court strikes down abortion law in landmark ruling.  (0.44316152597402597)\n",
      "\n",
      "SIMILAR SENTENCE: 'Planned Parenthood sues over Arizona abortion laws.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] South Korea court strikes down abortion law in landmark ruling.  (0.44112554112554114)\n",
      "\n",
      "Getting similar sentences for \"Houston reports 3 teens treated for vape-related lung issues.\" (1/1)\n",
      "------- Most common similar sentences for input text ------\n",
      "\n",
      "SIMILAR SENTENCE: 'Delaware probes 3 possible cases of vape-related lung issues.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Houston reports 3 teens treated for vape-related lung issues.  (0.46191197691197694)\n",
      "\n",
      "SIMILAR SENTENCE: '8 vaping lung illness cases reported in Oregon.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Houston reports 3 teens treated for vape-related lung issues.  (0.4280921459492888)\n",
      "\n",
      "SIMILAR SENTENCE: 'Kansas officials issue toxic algae alert for Lake Afton.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Houston reports 3 teens treated for vape-related lung issues.  (0.40540043290043287)\n",
      "\n",
      "Getting similar sentences for \"Obamacare \"cuts seniors’ Medicare.\" (1/1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Most common similar sentences for input text ------\n",
      "\n",
      "SIMILAR SENTENCE: '\"In 2010, Betty Sutton \"\"voted to destroy Medicare.\"'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Obamacare \"cuts seniors’ Medicare.  (0.6888888888888889)\n",
      "\n",
      "SIMILAR SENTENCE: '\"The new health care law \"\"will cut $500 billion from Medicare. That will hurt the quality of our care.\"'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Obamacare \"cuts seniors’ Medicare.  (0.5070684523809523)\n",
      "\n",
      "SIMILAR SENTENCE: 'Trump cites his support for Medicare, slams Medicare for All.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Obamacare \"cuts seniors’ Medicare.  (0.4186868686868687)\n",
      "\n",
      "Getting similar sentences for \"Prince Harry joins Elton John to launch HIV campaign targeting men.\" (1/1)\n",
      "------- Most common similar sentences for input text ------\n",
      "\n",
      "SIMILAR SENTENCE: 'Prince Harry, Elton John to launch coalition against HIV in men.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Prince Harry joins Elton John to launch HIV campaign targeting men.  (0.834090909090909)\n",
      "\n",
      "SIMILAR SENTENCE: 'Explainer: Do men fare worse with COVID-19?.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Prince Harry joins Elton John to launch HIV campaign targeting men.  (0.6293589743589744)\n",
      "\n",
      "SIMILAR SENTENCE: 'Trump launching campaign to end HIV epidemic in US by 2030.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Prince Harry joins Elton John to launch HIV campaign targeting men.  (0.5416375291375292)\n",
      "\n",
      "Getting similar sentences for \"Suicide kills one person every 40 seconds, says WHO.\" (1/1)\n",
      "------- Most common similar sentences for input text ------\n",
      "\n",
      "SIMILAR SENTENCE: '“Sunlight actually can kill the (novel coronavirus.)”'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Suicide kills one person every 40 seconds, says WHO.  (1.0)\n",
      "\n",
      "SIMILAR SENTENCE: 'Viral image Says the coronavirus isn’t new because “Lysol has it listed as one of the viruses that it kills.”'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Suicide kills one person every 40 seconds, says WHO.  (0.4705077330077331)\n",
      "\n",
      "SIMILAR SENTENCE: '\"Peter Courtney Says, \"\"Statistics indicate that one in eight children, and one in 18 adults in Oregon suffers from mental illness.\"'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Suicide kills one person every 40 seconds, says WHO.  (0.4647357272357272)\n",
      "\n",
      "Getting similar sentences for \"Hospitals plan for 'COVID cabanas,' conference rooms to house patients.\" (1/1)\n",
      "------- Most common similar sentences for input text ------\n",
      "\n",
      "SIMILAR SENTENCE: 'Hospital says it will be diverting trauma patients elsewhere.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Hospitals plan for 'COVID cabanas,' conference rooms to house patients.  (0.6930272108843537)\n",
      "\n",
      "SIMILAR SENTENCE: 'UW teaching hospital plan passes Washington House.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Hospitals plan for 'COVID cabanas,' conference rooms to house patients.  (0.6380411255411256)\n",
      "\n",
      "SIMILAR SENTENCE: 'Going into hospital far riskier than flying: WHO.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Hospitals plan for 'COVID cabanas,' conference rooms to house patients.  (0.6257111935683364)\n",
      "\n",
      "Getting similar sentences for \"People who undergo amputations can sign paperwork allowing them to take the removed limbs home after the procedure.\" (1/1)\n",
      "------- Most common similar sentences for input text ------\n",
      "\n",
      "SIMILAR SENTENCE: 'Left-handed people die younger than right-handed people.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] People who undergo amputations can sign paperwork allowing them to take the removed limbs home after the procedure.  (0.48386644219977554)\n",
      "\n",
      "SIMILAR SENTENCE: 'Italy nursing home ravaged by virus discloses 300 dead.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] People who undergo amputations can sign paperwork allowing them to take the removed limbs home after the procedure.  (0.426890756302521)\n",
      "\n",
      "SIMILAR SENTENCE: 'Actually, on some procedures, you're more likely to die if you're on Medicaid than if you're uninsured.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] People who undergo amputations can sign paperwork allowing them to take the removed limbs home after the procedure.  (0.3995707417582417)\n",
      "\n",
      "Getting similar sentences for \"Trump vows 'major' steps to aid U.S. economy amid coronavirus rise.\" (1/1)\n",
      "------- Most common similar sentences for input text ------\n",
      "\n",
      "SIMILAR SENTENCE: 'Trump says he went through ‘very routine physical’.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Trump vows 'major' steps to aid U.S. economy amid coronavirus rise.  (0.4236607142857143)\n",
      "\n",
      "SIMILAR SENTENCE: 'Fauci: ‘We’re not there yet’ on key steps to reopen economy.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Trump vows 'major' steps to aid U.S. economy amid coronavirus rise.  (0.4086111111111111)\n",
      "\n",
      "SIMILAR SENTENCE: 'In modernizing India, suicide is on the rise among young.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Trump vows 'major' steps to aid U.S. economy amid coronavirus rise.  (0.40073260073260075)\n",
      "\n",
      "Getting similar sentences for \"Indiana city in jeopardy of losing animal health business.\" (1/1)\n",
      "------- Most common similar sentences for input text ------\n",
      "\n",
      "SIMILAR SENTENCE: 'Drugmaker Lilly plans IPO for part of animal health business.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Indiana city in jeopardy of losing animal health business.  (0.6342147435897436)\n",
      "\n",
      "SIMILAR SENTENCE: 'Phelps honored for honesty on mental health, helping others.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Indiana city in jeopardy of losing animal health business.  (0.6102060707323865)\n",
      "\n",
      "SIMILAR SENTENCE: 'Indiana health officials say 11 more deaths from COVID-19.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Indiana city in jeopardy of losing animal health business.  (0.5517676767676767)\n",
      "\n",
      "Getting similar sentences for \"Heparin recalled in France, Italy, Denmark: report.\" (1/1)\n",
      "------- Most common similar sentences for input text ------\n",
      "\n",
      "SIMILAR SENTENCE: 'Hispanics in U.S. outlive whites, blacks: report.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Heparin recalled in France, Italy, Denmark: report.  (0.6121800993124522)\n",
      "\n",
      "SIMILAR SENTENCE: 'Bayer says reintroducing Diane-35 in France.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Heparin recalled in France, Italy, Denmark: report.  (0.460515873015873)\n",
      "\n",
      "SIMILAR SENTENCE: 'Macron warns France that coronavirus epidemic is coming.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Heparin recalled in France, Italy, Denmark: report.  (0.4242132867132867)\n",
      "\n",
      "Getting similar sentences for \"Page 92 of the House health care bill \"says specifically that people can't purchase private health insurance after a date certain.\" (1/1)\n",
      "------- Most common similar sentences for input text ------\n",
      "\n",
      "SIMILAR SENTENCE: '\"A public option for health care would end private insurance \"\"because the private insurance people will not be able to compete with a government option.\"'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Page 92 of the House health care bill \"says specifically that people can't purchase private health insurance after a date certain.  (0.6665373977873978)\n",
      "\n",
      "SIMILAR SENTENCE: 'Nonprofit has given $25M to Wyoming health care.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Page 92 of the House health care bill \"says specifically that people can't purchase private health insurance after a date certain.  (0.6658843681570954)\n",
      "\n",
      "SIMILAR SENTENCE: 'In the health care bill, we're now offering insurance for dogs.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Page 92 of the House health care bill \"says specifically that people can't purchase private health insurance after a date certain.  (0.6641666666666666)\n",
      "\n",
      "Getting similar sentences for \"A few drops of Visine brand eye drops taken internally will induce uncontrollable diarrhea.\" (1/1)\n",
      "------- Most common similar sentences for input text ------\n",
      "\n",
      "SIMILAR SENTENCE: 'Eli Lilly drops inhaled insulin program.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] A few drops of Visine brand eye drops taken internally will induce uncontrollable diarrhea.  (0.4118416305916306)\n",
      "\n",
      "SIMILAR SENTENCE: 'Teen birthrate hits new low amid overall drop.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] A few drops of Visine brand eye drops taken internally will induce uncontrollable diarrhea.  (0.35300532800532797)\n",
      "\n",
      "SIMILAR SENTENCE: ''It's just impossible': tracing contacts takes backseat as virus spreads.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] A few drops of Visine brand eye drops taken internally will induce uncontrollable diarrhea.  (0.3064435564435564)\n",
      "\n",
      "Getting similar sentences for \"Nebraska Sen. Bolz to run for 1st Congressional District.\" (1/1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Most common similar sentences for input text ------\n",
      "\n",
      "SIMILAR SENTENCE: 'Ebola spreads to remote, militia-run Congo territory.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Nebraska Sen. Bolz to run for 1st Congressional District.  (0.5104166666666666)\n",
      "\n",
      "SIMILAR SENTENCE: 'Snow day? No, it’s illness _ again _ in school district.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Nebraska Sen. Bolz to run for 1st Congressional District.  (0.49615384615384617)\n",
      "\n",
      "SIMILAR SENTENCE: '“Some states, like Montana and Nebraska, are getting more than $300,000 in federal stimulus money per reported COVID-19 case. New York is the hardest-hit state and yet we are getting only about $12,000 per case.”'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Nebraska Sen. Bolz to run for 1st Congressional District.  (0.45780303030303027)\n",
      "\n",
      "Getting similar sentences for \"Charlie Crist Says Rick Scott signed \"laws requiring mandatory ultrasounds and restricting access to abortion even in cases of rape or incest.\" (1/1)\n",
      "------- Most common similar sentences for input text ------\n",
      "\n",
      "SIMILAR SENTENCE: '“The (COVID-19) cases are going up, but it's because the testing is going up.”'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Charlie Crist Says Rick Scott signed \"laws requiring mandatory ultrasounds and restricting access to abortion even in cases of rape or incest.  (0.5807583211994977)\n",
      "\n",
      "SIMILAR SENTENCE: 'Rick Scott vetoed funding for 30 Florida rape crisis centers. Scott said the centers weren’t needed.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Charlie Crist Says Rick Scott signed \"laws requiring mandatory ultrasounds and restricting access to abortion even in cases of rape or incest.  (0.5094241466036338)\n",
      "\n",
      "SIMILAR SENTENCE: '\"Upwards of 90 percent\"\" of women seeking an abortion decide not to have an abortion after seeing an ultrasound.\"'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Charlie Crist Says Rick Scott signed \"laws requiring mandatory ultrasounds and restricting access to abortion even in cases of rape or incest.  (0.44403952528952534)\n",
      "\n",
      "Getting similar sentences for \"UN: Nearly a half-billion in Asia-Pacific still going hungry.\" (1/1)\n",
      "------- Most common similar sentences for input text ------\n",
      "\n",
      "SIMILAR SENTENCE: '“The (COVID-19) cases are going up, but it's because the testing is going up.”'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] UN: Nearly a half-billion in Asia-Pacific still going hungry.  (0.5102272727272728)\n",
      "\n",
      "SIMILAR SENTENCE: 'Philippines coronavirus testing to be stepped up soon: WHO.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] UN: Nearly a half-billion in Asia-Pacific still going hungry.  (0.5)\n",
      "\n",
      "SIMILAR SENTENCE: 'Nearly 700 from LA universities still in measles quarantine.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] UN: Nearly a half-billion in Asia-Pacific still going hungry.  (0.4650865800865801)\n",
      "\n",
      "Getting similar sentences for \"New York hospital sends some 'borderline' COVID-19 patients home with oxygen monitors.\" (1/1)\n",
      "------- Most common similar sentences for input text ------\n",
      "\n",
      "SIMILAR SENTENCE: 'Hospital says it will be diverting trauma patients elsewhere.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] New York hospital sends some 'borderline' COVID-19 patients home with oxygen monitors.  (0.7003306878306879)\n",
      "\n",
      "SIMILAR SENTENCE: 'Going into hospital far riskier than flying: WHO.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] New York hospital sends some 'borderline' COVID-19 patients home with oxygen monitors.  (0.6153036778036778)\n",
      "\n",
      "SIMILAR SENTENCE: 'Carmat artificial heart patient in good condition: hospital.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] New York hospital sends some 'borderline' COVID-19 patients home with oxygen monitors.  (0.4752238502238503)\n",
      "\n",
      "Getting similar sentences for \"Fitness experts separate folklore from fact.\" (1/1)\n",
      "------- Most common similar sentences for input text ------\n",
      "\n",
      "SIMILAR SENTENCE: 'Fitness experts extol machine-less workout.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Fitness experts separate folklore from fact.  (0.5797461797461798)\n",
      "\n",
      "SIMILAR SENTENCE: 'Checking up on your fitness form.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Fitness experts separate folklore from fact.  (0.47685185185185186)\n",
      "\n",
      "SIMILAR SENTENCE: 'Posthumous sperm donation should be allowed, say UK experts.'; COUNT: 1\n",
      "ORIGINAL SENTENCES:\n",
      "   - [***FULL INPUT TEXT***] Fitness experts separate folklore from fact.  (0.4727272727272728)\n",
      "\n",
      "Getting similar sentences for \"Okposo says he’s healthy after concussion put him in ICU.\" (1/1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-5832a5091d65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mprocess_input_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-72-b6d4aa89ca69>\u001b[0m in \u001b[0;36mprocess_input_text\u001b[0;34m(input_text, classifier)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0minput_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0munique_similar_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-58cdc49a9ce1>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x_test)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;31m# Iterate over training examples and find sentence similarity scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m                 \u001b[0mscore_to_index_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-58cdc49a9ce1>\u001b[0m in \u001b[0;36mdocument_similarity\u001b[0;34m(self, doc1, doc2)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;34m\"\"\"Finds the similarity between doc1 and doc2\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0msynsets1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_to_synsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0msynsets2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_to_synsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-58cdc49a9ce1>\u001b[0m in \u001b[0;36mdoc_to_synsets\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \"\"\"\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0map_russian_model_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, load)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             AP_MODEL_LOC = \"file:\" + str(\n\u001b[0;32m--> 168\u001b[0;31m                 \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"taggers/averaged_perceptron_tagger/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mPICKLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             )\n\u001b[1;32m    170\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAP_MODEL_LOC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;31m# Is the path item a directory or is resource_name an absolute path?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath_\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mzipfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                 \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl2pathname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/genericpath.py\u001b[0m in \u001b[0;36misdir\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS_ISDIR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_data_file_path = \"/Users/gwenythportillowightman/OneDrive - Johns Hopkins/fall-2022/interpretable_ml_design/PUBHEALTH/test.tsv\"          \n",
    "\n",
    "test_df = prepare_df(test_data_file_path)\n",
    "\n",
    "test_data_subset = test_df[:10]\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    input_text = row['text']\n",
    "    process_input_text(input_text, classifier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ca4458",
   "metadata": {},
   "source": [
    "### Examples from [User Study Examples](https://docs.google.com/spreadsheets/d/1BF-PR27TVwq9P6pcZQ9eHCOuQVP9nT989nmta5CbcGM/edit#gid=63935314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949e6e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting similar sentences for \"You can get HIV from fruits or vegetables from other countries.\" (1/1)\n"
     ]
    }
   ],
   "source": [
    "user_study_examples_file_path = '/Users/gwenythportillowightman/OneDrive - Johns Hopkins/fall-2022/interpretable_ml_design/user_study_examples.csv'\n",
    "\n",
    "USE_df = pd.read_csv(user_study_examples_file_path)\n",
    "\n",
    "for index, row in USE_df.iterrows():\n",
    "    input_text = row['Claims']\n",
    "    process_input_text(input_text, classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
