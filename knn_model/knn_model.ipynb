{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd983992",
   "metadata": {},
   "source": [
    "# Reddit HealthAdviceChecKer Bot Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5fe1d7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb34f8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package genesis to\n",
      "[nltk_data]     /Users/gwenythportillowightman/nltk_data...\n",
      "[nltk_data]   Package genesis is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gwenythportillowightman/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/gwenythportillowightman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/gwenythportillowightman/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import genesis\n",
    "nltk.download('genesis')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "genesis_ic = wn.ic(genesis, False, 0.0)\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfcf723",
   "metadata": {},
   "source": [
    "## Filtering the dataset to health claims only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1b29755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format tags columns in df\n",
    "\n",
    "def format_tags(df):\n",
    "    tags = []\n",
    "    tag_lists = []\n",
    "\n",
    "    for subjects in df.subjects:\n",
    "        if type(subjects) is str:\n",
    "            s = subjects.split(\",\")\n",
    "        else:\n",
    "            if type(subjects) is list:\n",
    "                s = subjects\n",
    "            else:\n",
    "                s = []\n",
    "        s = [t.lstrip().rstrip() for t in s]\n",
    "        tag_lists.append(s)\n",
    "        for tag in s:\n",
    "            tags.append(tag)\n",
    "    df['tags'] = tag_lists\n",
    "    return df, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b376ef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select claims from relevant categories\n",
    "health_tags = ['Health', 'Health News', \"Health Care\", 'Medical', 'Public Health']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a872b84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for masking dataframe with relevant tags\n",
    "def health(x):\n",
    "    for t in health_tags:\n",
    "        if t in x:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb9fc15",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae45172e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df (3596, 12)\n"
     ]
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "data_file_path = \"/Users/gwenythportillowightman/OneDrive - Johns Hopkins/fall-2022/interpretable_ml_design/PUBHEALTH/train.tsv\"          \n",
    "\n",
    "df = pd.read_csv(data_file_path, sep='\\t')\n",
    "df, df_tags = format_tags(df)\n",
    "\n",
    "mask = df['tags'].apply(lambda x: health(x))\n",
    "df = df[mask]\n",
    "\n",
    "# text_col contains the column name of where claims are found\n",
    "# answer_col contains the column name of where post labels (true, false, etc.) are found\n",
    "text_col = \"text\"\n",
    "answer_col = \"label\"\n",
    "\n",
    "# Rename the claim column to \"text\" and label column to \"label_categorical\"\n",
    "df.rename(columns = {\"claim\": \"text\", \"label\": \"label_categorical\"}, inplace = True)\n",
    "# Make the categorical labels into numbers (0, 1, 2, 3)\n",
    "df[\"label\"] = pd.factorize(df[\"label_categorical\"])[0]\n",
    "df = df.dropna(subset=[text_col])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Make a copy of the 'text' column\n",
    "df['text_original'] = df['text']\n",
    "\n",
    "print(f\"Shape of df {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a958719e",
   "metadata": {},
   "source": [
    "### Prepare to preprocess text claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a353a197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gwenythportillowightman/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "nltk.download('stopwords')\n",
    "s = stopwords.words('english')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    text = [ps.lemmatize(word) for word in text if not word in s]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0d9db0",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3350a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed claims\n",
      "0    britain reveal trial criterion coronavirus ant...\n",
      "1    u say result encouraging healthcare delivery r...\n",
      "2    latest trial j j talc litigation get way calif...\n",
      "3       democrat hoping flip house trash talking trump\n",
      "4        sex tech woman led startup pop ce gadget show\n",
      "5                             waxed apple cause cancer\n",
      "6    rhode island become second state mandate vacci...\n",
      "7    brazil city lurch lockdown amid virus crisis r...\n",
      "8    slovakia new government sharply ramp coronavir...\n",
      "9                       coronavirus simply common cold\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "ps = nltk.wordnet.WordNetLemmatizer()\n",
    "for i in range(df.shape[0]):\n",
    "    text = df.loc[i,'text']\n",
    "    text = preprocess_text(text)\n",
    "    df.loc[i, 'text'] = text\n",
    "    X_train = df['text']\n",
    "y_train = df['label']\n",
    "\n",
    "print(\"Preprocessed claims\")\n",
    "print(df['text'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d435b029",
   "metadata": {},
   "source": [
    "## KNN Model\n",
    "\n",
    "Returns the top k most similar sentences from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f7453eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_Model():\n",
    "    def __init__(self, k=3, distance_type = 'path'):\n",
    "        self.k = k\n",
    "        self.distance_type = distance_type\n",
    "\n",
    "    # This function is used for training\n",
    "    def fit(self, x_train, y_train):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def split_input(self, input_sentence):\n",
    "        test_corpus = []\n",
    "        \n",
    "        # Preprocess the full x_test input\n",
    "        input_sentence_copy = copy.deepcopy(input_sentence)\n",
    "        input_sentence_copy = preprocess_text(input_sentence_copy)\n",
    "        \n",
    "        # Preprocess sentences of the input\n",
    "        sentences = sent_tokenize(input_sentence)\n",
    "        for sentence in sentences:\n",
    "            sentence = preprocess_text(sentence)\n",
    "            test_corpus.append(sentence)\n",
    "            \n",
    "        if len(test_corpus) > 1:\n",
    "            test_corpus.append(input_sentence_copy)\n",
    "        \n",
    "        return test_corpus\n",
    "\n",
    "    # Returns the k most similar sentences for the input sentence\n",
    "    # Predict returns the n similar sentences as a list of tuples [(sentence, score), (sentence, score), ...]\n",
    "    # Takes in only one input at a time\n",
    "    def predict(self, x_test):\n",
    "        test_corpus = self.split_input(x_test)\n",
    "            \n",
    "        self.x_test = test_corpus\n",
    "    \n",
    "        # {score: [(index of sentence in `test_corpus`, similar sentence index in `dataset`)], ...}\n",
    "        all_top_scores_dict = {}\n",
    "\n",
    "        # Iterate over sentences of the input\n",
    "        for i in range(len(self.x_test)):\n",
    "            print(f\"Getting similar sentences for \\\"{self.x_test[i]}\\\" ({i+1}/{len(self.x_test)})\")\n",
    "            \n",
    "            # {score: similar_sentence_index_in_`dataset`, ...}\n",
    "            score_to_index_dict = {}\n",
    "            \n",
    "            # Iterate over training examples and find sentence similarity scores\n",
    "            for j in range(self.x_train.shape[0]): \n",
    "                score = self.document_similarity(self.x_test[i], self.x_train[j])\n",
    "                score_to_index_dict[score] = j\n",
    "\n",
    "            sorted_scores = list(score_to_index_dict.keys())\n",
    "            sorted_scores.sort(reverse=True)\n",
    "\n",
    "            # Get the top k similar sentences for the current sentence (x_test[i])\n",
    "            for k in range(self.k):\n",
    "                score = sorted_scores[k]\n",
    "                \n",
    "                if score in all_top_scores_dict:\n",
    "                    all_top_scores_dict[score].append( (i, score_to_index_dict[score]) )\n",
    "                else:\n",
    "                    all_top_scores_dict[score] = [ (i, score_to_index_dict[score]) ]\n",
    "                    \n",
    "        # Get the top k scoring sentences and similar sentences from all_top_scores_dict\n",
    "        sorted_scores = list(all_top_scores_dict.keys())\n",
    "        sorted_scores.sort(reverse=True)\n",
    "        \n",
    "        # [ ((index_of_sentence_in_input, index_of_similar_sentence_in_`dataset`), score), ...]\n",
    "        similar_texts_list = []\n",
    "        \n",
    "        for k in range(self.k):\n",
    "            score = sorted_scores[k]\n",
    "            new_tuple = (all_top_scores_dict[score], score)\n",
    "            similar_texts_list.append(new_tuple)\n",
    "\n",
    "        return similar_texts_list\n",
    "    \n",
    "    def convert_tag(self, tag):\n",
    "        \"\"\"Convert the tag given by nltk.pos_tag to the tag used by wordnet.synsets\"\"\"\n",
    "        tag_dict = {'N': 'n', 'J': 'a', 'R': 'r', 'V': 'v'}\n",
    "        try:\n",
    "            return tag_dict[tag[0]]\n",
    "        except KeyError:\n",
    "            return None\n",
    "\n",
    "    def doc_to_synsets(self, doc):\n",
    "        \"\"\"\n",
    "            Returns a list of synsets in document.\n",
    "            Tokenizes and tags the words in the document doc.\n",
    "            Then finds the first synset for each word/tag combination.\n",
    "        If a synset is not found for that combination it is skipped.\n",
    "\n",
    "        Args:\n",
    "            doc: string to be converted\n",
    "\n",
    "        Returns:\n",
    "            list of synsets\n",
    "        \"\"\"\n",
    "        tokens = word_tokenize(doc+' ')\n",
    "        \n",
    "        l = []\n",
    "        tags = nltk.pos_tag([tokens[0] + ' ']) if len(tokens) == 1 else nltk.pos_tag(tokens)\n",
    "        \n",
    "        for token, tag in zip(tokens, tags):\n",
    "            syntag = self.convert_tag(tag[1])\n",
    "            syns = wn.synsets(token, syntag)\n",
    "            if (len(syns) > 0):\n",
    "                l.append(syns[0])\n",
    "        return l  \n",
    "    \n",
    "    def similarity_score(self, s1, s2, distance_type = 'path'):\n",
    "        \"\"\"\n",
    "        Calculate the normalized similarity score of s1 onto s2\n",
    "        For each synset in s1, finds the synset in s2 with the largest similarity value.\n",
    "        Sum of all of the largest similarity values and normalize this value by dividing it by the\n",
    "        number of largest similarity values found.\n",
    "\n",
    "        Args:\n",
    "          s1, s2: list of synsets from doc_to_synsets\n",
    "\n",
    "        Returns:\n",
    "          normalized similarity score of s1 onto s2\n",
    "        \"\"\"\n",
    "        s1_largest_scores = []\n",
    "\n",
    "        for i, s1_synset in enumerate(s1, 0):\n",
    "            max_score = 0\n",
    "            for s2_synset in s2:\n",
    "                if distance_type == 'path':\n",
    "                    score = s1_synset.path_similarity(s2_synset, simulate_root = False)\n",
    "                else:\n",
    "                    score = s1_synset.wup_similarity(s2_synset)                  \n",
    "                if score != None:\n",
    "                    if score > max_score:\n",
    "                        max_score = score\n",
    "\n",
    "            if max_score != 0:\n",
    "                s1_largest_scores.append(max_score)\n",
    "\n",
    "        mean_score = np.mean(s1_largest_scores)\n",
    "\n",
    "        return mean_score \n",
    "    \n",
    "    def document_similarity(self, doc1, doc2):\n",
    "        \"\"\"Finds the similarity between doc1 and doc2\"\"\"\n",
    "\n",
    "        synsets1 = self.doc_to_synsets(doc1)\n",
    "        synsets2 = self.doc_to_synsets(doc2)\n",
    "          \n",
    "        return (self.similarity_score(synsets1, synsets2) + self.similarity_score(synsets2, synsets1)) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ad9d1e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting similar sentences for \"coated toxic chemical\" (1/3)\n",
      "Getting similar sentences for \"dryer sheet one worst thing chemical allergy standpoint\" (2/3)\n",
      "Getting similar sentences for \"coated toxic chemical dryer sheet one worst thing chemical allergy standpoint\" (3/3)\n",
      "\n",
      "Top 15 similar examples:\n",
      "Original sentence: coated toxic chemical\n",
      "Similar sentence: English Channel dolphins carry ‘toxic cocktail’ of chemicals.\n",
      "Label: true\n",
      "Score: 0.7743055555555556\n",
      "\n",
      "Original sentence: coated toxic chemical\n",
      "Similar sentence: Born Basic Anti-Bac hand sanitizer was recalled in the U.S. after being found to contain methanol, a poisonous chemical.\n",
      "Label: true\n",
      "Score: 0.7026515151515151\n",
      "\n",
      "Original sentence: coated toxic chemical\n",
      "Similar sentence: States, military clash on cleanup of toxic chemicals.\n",
      "Label: true\n",
      "Score: 0.6563956876456877\n",
      "\n",
      "Original sentence: coated toxic chemical\n",
      "Similar sentence: Residents near Harvey-damaged chemical plant wary of water.\n",
      "Label: true\n",
      "Score: 0.6563131313131313\n",
      "\n",
      "Original sentence: coated toxic chemical\n",
      "Similar sentence: \"Doctors are now warning parents to never use baby wipes because they contain a \"\"chemical\"\" called methylisothiazolinone.\"\n",
      "Label: false\n",
      "Score: 0.634090909090909\n",
      "\n",
      "Original sentence: coated toxic chemical\n",
      "Similar sentence: Pampers Dry Max diapers commonly cause severe diaper rash and chemical burns on infants.\n",
      "Label: false\n",
      "Score: 0.6176184926184927\n",
      "\n",
      "Original sentence: coated toxic chemical dryer sheet one worst thing chemical allergy standpoint\n",
      "Similar sentence: Dryer sheets permeated with fabric softener contain at least seven dangerous toxic chemicals.\n",
      "Label: false\n",
      "Score: 0.6143089549339549\n",
      "\n",
      "Original sentence: dryer sheet one worst thing chemical allergy standpoint\n",
      "Similar sentence: \"The coronavirus \"\"snuck up on us,” adding that it is “a very unforeseen thing.”\"\n",
      "Label: false\n",
      "Score: 0.6128427128427129\n",
      "\n",
      "Original sentence: coated toxic chemical\n",
      "Similar sentence: A new analysis of the residue on common American foods showed high levels of the herbicide glyphosate and Monsanto, the EPA, and the FDA are in cahoots to silence word of the chemical’s harmful effects and its high concentration in foods.\n",
      "Label: false\n",
      "Score: 0.6115829772079772\n",
      "\n",
      "Original sentence: dryer sheet one worst thing chemical allergy standpoint\n",
      "Similar sentence: Dryer sheets permeated with fabric softener contain at least seven dangerous toxic chemicals.\n",
      "Label: false\n",
      "Score: 0.5467102342102341\n",
      "\n",
      "Original sentence: coated toxic chemical dryer sheet one worst thing chemical allergy standpoint\n",
      "Similar sentence: English Channel dolphins carry ‘toxic cocktail’ of chemicals.\n",
      "Label: true\n",
      "Score: 0.43461920024420025\n",
      "\n",
      "Original sentence: dryer sheet one worst thing chemical allergy standpoint\n",
      "Similar sentence: Ebola gives U.S. 'preppers' another reason to prepare for worst.\n",
      "Label: true\n",
      "Score: 0.3850949397824398\n",
      "\n",
      "Original sentence: coated toxic chemical\n",
      "Similar sentence: Right now, China controls 80% of all ingredients and raw materials going into our generic prescription drugs.\n",
      "Label: false\n",
      "Score: 0.34197802197802196\n",
      "\n",
      "Original sentence: coated toxic chemical\n",
      "Similar sentence: European labs to receive control material to spot false negative coronavirus tests.\n",
      "Label: true\n",
      "Score: 0.3368686868686869\n",
      "\n",
      "Original sentence: dryer sheet one worst thing chemical allergy standpoint\n",
      "Similar sentence: Know what to say when postpartum depression hits a loved one.\n",
      "Label: true\n",
      "Score: 0.3345689033189033\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k_value = 15\n",
    "\n",
    "classifier = KNN_Model(k=k_value, distance_type='path')\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "input_text = 'They are coated in toxic chemicals. Dryer sheets are one of the very worst things from a chemical allergy standpoint.'\n",
    "\n",
    "input_sentences = classifier.split_input(input_text)\n",
    "\n",
    "y_pred = classifier.predict(input_text)\n",
    "\n",
    "print()\n",
    "print(f\"Top {k_value} similar examples:\")\n",
    "\n",
    "# Print out the most similar sentences\n",
    "for i, result in enumerate (y_pred):\n",
    "    original_sentence_index = result[0][0][0]\n",
    "    \n",
    "    if original_sentence_index == len(input_sentences):\n",
    "        original_sentence = input_text\n",
    "    else:\n",
    "        original_sentence = input_sentences[original_sentence_index]\n",
    "    \n",
    "    similar_sentence_index = result[0][0][1]\n",
    "    similar_sentence_data = df.iloc[[similar_sentence_index]].values.tolist()[0]\n",
    "    \n",
    "    text_original_column_index = 11\n",
    "    label_categorical_column_index = 7\n",
    "    \n",
    "    score = result[1]\n",
    "    \n",
    "    print(f'Original sentence: {original_sentence}')\n",
    "    print(f'Similar sentence: {similar_sentence_data[text_original_column_index]}')\n",
    "    print(f'Label: {similar_sentence_data[label_categorical_column_index]}')\n",
    "    print(f'Score: {score}')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
