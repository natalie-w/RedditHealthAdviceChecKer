{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb34f8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package genesis to\n",
      "[nltk_data]     /Users/gwenythportillowightman/nltk_data...\n",
      "[nltk_data]   Package genesis is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gwenythportillowightman/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/gwenythportillowightman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/gwenythportillowightman/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import genesis\n",
    "nltk.download('genesis')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "genesis_ic = wn.ic(genesis, False, 0.0)\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1b29755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format tags columns in df\n",
    "\n",
    "def format_tags(df):\n",
    "    tags = []\n",
    "    tag_lists = []\n",
    "\n",
    "    for subjects in df.subjects:\n",
    "        if type(subjects) is str:\n",
    "            s = subjects.split(\",\")\n",
    "        else:\n",
    "            if type(subjects) is list:\n",
    "                s = subjects\n",
    "            else:\n",
    "                s = []\n",
    "        s = [t.lstrip().rstrip() for t in s]\n",
    "        tag_lists.append(s)\n",
    "        for tag in s:\n",
    "            tags.append(tag)\n",
    "    df['tags'] = tag_lists\n",
    "    return df, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b376ef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select claims from relevant categories\n",
    "health_tags = ['Health', 'Health News', \"Health Care\", 'Medical', 'Public Health']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a872b84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for masking dataframe with relevant tags\n",
    "def health(x):\n",
    "    for t in health_tags:\n",
    "        if t in x:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae45172e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df (3596, 12)\n"
     ]
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "data_file_path = \"/Users/gwenythportillowightman/OneDrive - Johns Hopkins/fall-2022/interpretable_ml_design/PUBHEALTH/train.tsv\"          \n",
    "\n",
    "df = pd.read_csv(data_file_path, sep='\\t')\n",
    "df, df_tags = format_tags(df)\n",
    "\n",
    "mask = df['tags'].apply(lambda x: health(x))\n",
    "df = df[mask]\n",
    "\n",
    "# text_col contains the column name of where claims are found\n",
    "# answer_col contains the column name of where post labels (true, false, etc.) are found\n",
    "text_col = \"text\"\n",
    "answer_col = \"label\"\n",
    "\n",
    "# Rename the claim column to \"text\" and label column to \"label_categorical\"\n",
    "df.rename(columns = {\"claim\": \"text\", \"label\": \"label_categorical\"}, inplace = True)\n",
    "# Make the categorical labels into numbers (0, 1, 2, 3)\n",
    "df[\"label\"] = pd.factorize(df[\"label_categorical\"])[0]\n",
    "df = df.dropna(subset=[text_col])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Make a copy of the 'text' column\n",
    "df['text_original'] = df['text']\n",
    "\n",
    "print(f\"Shape of df {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41bef5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    text = [ps.lemmatize(word) for word in text if not word in s]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3350a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gwenythportillowightman/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed claims\n",
      "0    britain reveal trial criterion coronavirus ant...\n",
      "1    u say result encouraging healthcare delivery r...\n",
      "2    latest trial j j talc litigation get way calif...\n",
      "3       democrat hoping flip house trash talking trump\n",
      "4        sex tech woman led startup pop ce gadget show\n",
      "5                             waxed apple cause cancer\n",
      "6    rhode island become second state mandate vacci...\n",
      "7    brazil city lurch lockdown amid virus crisis r...\n",
      "8    slovakia new government sharply ramp coronavir...\n",
      "9                       coronavirus simply common cold\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "nltk.download('stopwords')\n",
    "s = stopwords.words('english')\n",
    "\n",
    "ps = nltk.wordnet.WordNetLemmatizer()\n",
    "for i in range(df.shape[0]):\n",
    "    text = df.loc[i,'text']\n",
    "    text = preprocess_text(text)\n",
    "    df.loc[i, 'text'] = text\n",
    "    X_train = df['text']\n",
    "y_train = df['label']\n",
    "\n",
    "print(\"preprocessed claims\")\n",
    "print(df['text'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7453eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_NLC_Classifer():\n",
    "    def __init__(self, k=3, distance_type = 'path'):\n",
    "        self.k = k\n",
    "        self.distance_type = distance_type\n",
    "\n",
    "    # This function is used for training\n",
    "    def fit(self, x_train, y_train):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    # This function runs the K(n) nearest neighbour algorithm and\n",
    "    # returns the label with closest match. \n",
    "    # Predict returns the n similar sentences as a list of tuples [(sentence, score), (sentence, score), ...]\n",
    "    # Takes in only one input at a time\n",
    "    def predict(self, x_test):\n",
    "        test_corpus = []\n",
    "        \n",
    "        # Preprocess the full x_test input\n",
    "        x_test_copy = copy.deepcopy(x_test)\n",
    "        x_test_copy = preprocess_text(x_test_copy)\n",
    "        \n",
    "        # Preprocess sentences of the input\n",
    "        sentences = sent_tokenize(x_test)\n",
    "        for sentence in sentences:\n",
    "            sentence = preprocess_text(sentence)\n",
    "            test_corpus.append(sentence)\n",
    "            \n",
    "        test_corpus.append(x_test_copy)\n",
    "            \n",
    "        self.x_test = test_corpus\n",
    "    \n",
    "        # {score: [(index of sentence in `test_corpus`, similar sentence index in `dataset`)], ...}\n",
    "        all_top_scores_dict = {}\n",
    "\n",
    "        # Iterate over sentences of the input\n",
    "        for i in range(len(self.x_test)):\n",
    "            print(f\"Getting similar sentences for {self.x_test[i]}\")\n",
    "            \n",
    "            # {score: similar_sentence_index_in_`dataset`, ...}\n",
    "            score_to_index_dict = {}\n",
    "            \n",
    "            # Iterate over training examples and find sentence similarity scores\n",
    "            for j in range(self.x_train.shape[0]): \n",
    "                score = self.document_similarity(self.x_test[i], self.x_train[j])\n",
    "                score_to_index_dict[score] = j\n",
    "\n",
    "            sorted_scores = list(score_to_index_dict.keys())\n",
    "            sorted_scores.sort(reverse=True)\n",
    "\n",
    "            # Get the top k similar sentences for the current sentence (x_test[i])\n",
    "            for k in range(self.k):\n",
    "                score = sorted_scores[k]\n",
    "                \n",
    "                if score in all_top_scores_dict:\n",
    "                    all_top_scores_dict[score].append( (i, score_to_index_dict[score]) )\n",
    "                else:\n",
    "                    all_top_scores_dict[score] = [ (i, score_to_index_dict[score]) ]\n",
    "                    \n",
    "        # Get the top k scoring sentences and similar sentences from all_top_scores_dict\n",
    "        sorted_scores = list(all_top_scores_dict.keys())\n",
    "        sorted_scores.sort(reverse=True)\n",
    "        \n",
    "        # [ ((index_of_sentence_in_input, index_of_similar_sentence_in_`dataset`), score), ...]\n",
    "        similar_texts_list = []\n",
    "        \n",
    "        for k in range(self.k):\n",
    "            score = sorted_scores[k]\n",
    "            new_tuple = (all_top_scores_dict[score], score)\n",
    "            similar_texts_list.append(new_tuple)\n",
    "\n",
    "        return similar_texts_list\n",
    "    \n",
    "    def convert_tag(self, tag):\n",
    "        \"\"\"Convert the tag given by nltk.pos_tag to the tag used by wordnet.synsets\"\"\"\n",
    "        tag_dict = {'N': 'n', 'J': 'a', 'R': 'r', 'V': 'v'}\n",
    "        try:\n",
    "            return tag_dict[tag[0]]\n",
    "        except KeyError:\n",
    "            return None\n",
    "\n",
    "    def doc_to_synsets(self, doc):\n",
    "        \"\"\"\n",
    "            Returns a list of synsets in document.\n",
    "            Tokenizes and tags the words in the document doc.\n",
    "            Then finds the first synset for each word/tag combination.\n",
    "        If a synset is not found for that combination it is skipped.\n",
    "\n",
    "        Args:\n",
    "            doc: string to be converted\n",
    "\n",
    "        Returns:\n",
    "            list of synsets\n",
    "        \"\"\"\n",
    "        tokens = word_tokenize(doc+' ')\n",
    "        \n",
    "        l = []\n",
    "        tags = nltk.pos_tag([tokens[0] + ' ']) if len(tokens) == 1 else nltk.pos_tag(tokens)\n",
    "        \n",
    "        for token, tag in zip(tokens, tags):\n",
    "            syntag = self.convert_tag(tag[1])\n",
    "            syns = wn.synsets(token, syntag)\n",
    "            if (len(syns) > 0):\n",
    "                l.append(syns[0])\n",
    "        return l  \n",
    "    \n",
    "    def similarity_score(self, s1, s2, distance_type = 'path'):\n",
    "        \"\"\"\n",
    "        Calculate the normalized similarity score of s1 onto s2\n",
    "        For each synset in s1, finds the synset in s2 with the largest similarity value.\n",
    "        Sum of all of the largest similarity values and normalize this value by dividing it by the\n",
    "        number of largest similarity values found.\n",
    "\n",
    "        Args:\n",
    "          s1, s2: list of synsets from doc_to_synsets\n",
    "\n",
    "        Returns:\n",
    "          normalized similarity score of s1 onto s2\n",
    "        \"\"\"\n",
    "        s1_largest_scores = []\n",
    "\n",
    "        for i, s1_synset in enumerate(s1, 0):\n",
    "            max_score = 0\n",
    "            for s2_synset in s2:\n",
    "                if distance_type == 'path':\n",
    "                    score = s1_synset.path_similarity(s2_synset, simulate_root = False)\n",
    "                else:\n",
    "                    score = s1_synset.wup_similarity(s2_synset)                  \n",
    "                if score != None:\n",
    "                    if score > max_score:\n",
    "                        max_score = score\n",
    "\n",
    "            if max_score != 0:\n",
    "                s1_largest_scores.append(max_score)\n",
    "\n",
    "        mean_score = np.mean(s1_largest_scores)\n",
    "\n",
    "        return mean_score \n",
    "    \n",
    "    def document_similarity(self, doc1, doc2):\n",
    "        \"\"\"Finds the similarity between doc1 and doc2\"\"\"\n",
    "\n",
    "        synsets1 = self.doc_to_synsets(doc1)\n",
    "        synsets2 = self.doc_to_synsets(doc2)\n",
    "          \n",
    "        return (self.similarity_score(synsets1, synsets2) + self.similarity_score(synsets2, synsets1)) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad9d1e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting similar sentences for coated toxic chemical\n",
      "Getting similar sentences for dryer sheet one worst thing chemical allergy standpoint\n",
      "Getting similar sentences for coated toxic chemical dryer sheet one worst thing chemical allergy standpoint\n",
      "[([(0, 2041)], 0.7743055555555556), ([(0, 3378)], 0.7026515151515151), ([(0, 680)], 0.6563956876456877)]\n",
      "\n",
      "Top 3 similar examples:\n",
      "Original sentence: English Channel dolphins carry ‘toxic cocktail’ of chemicals.\n",
      "Label: true\n",
      "\n",
      "Original sentence: Born Basic Anti-Bac hand sanitizer was recalled in the U.S. after being found to contain methanol, a poisonous chemical.\n",
      "Label: true\n",
      "\n",
      "Original sentence: States, military clash on cleanup of toxic chemicals.\n",
      "Label: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Train the Classifier\n",
    "classifier = KNN_NLC_Classifer(k=3, distance_type='path')\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "input = 'They are coated in toxic chemicals. Dryer sheets are one of the very worst things from a chemical allergy standpoint.'\n",
    "\n",
    "y_pred = classifier.predict(input)\n",
    "print(y_pred)\n",
    "print()\n",
    "print(\"Top 3 similar examples:\")\n",
    "\n",
    "for i, result in enumerate (y_pred):\n",
    "#     print(f\"index of sentence in input & similar sentence index in dataset: {result[0]}, score: {result[1]}\")\n",
    "    original_sentence_index = result[0][0][0]\n",
    "    similar_sentence_index = result[0][0][1]\n",
    "    similar_sentence_data = df.iloc[[similar_sentence_index]].values.tolist()[0]\n",
    "    text_original_column_index = 11\n",
    "    label_categorical_column_index = 7\n",
    "    print(f'Original sentence: {similar_sentence_data[text_original_column_index]}')\n",
    "    print(f'Label: {similar_sentence_data[label_categorical_column_index]}')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
