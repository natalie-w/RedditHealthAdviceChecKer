{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb34f8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package genesis to\n",
      "[nltk_data]     /Users/nataliewang/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/genesis.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nataliewang/nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nataliewang/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nataliewang/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import genesis\n",
    "nltk.download('genesis')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "genesis_ic = wn.ic(genesis, False, 0.0)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6db3348",
   "metadata": {},
   "source": [
    "# K=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6366adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class KNN_NLC_Classifer():\n",
    "#     def __init__(self, k=1, distance_type = 'path'):\n",
    "#         self.k = k\n",
    "#         self.distance_type = distance_type\n",
    "\n",
    "#     # This function is used for training\n",
    "#     def fit(self, x_train, y_train):\n",
    "#         self.x_train = x_train\n",
    "#         self.y_train = y_train\n",
    "\n",
    "#     # This function runs the K(1) nearest neighbour algorithm and\n",
    "#     # returns the label with closest match. \n",
    "#     def predict(self, x_test):\n",
    "#         self.x_test = x_test\n",
    "#         y_predict = []\n",
    "\n",
    "#         for i in range(len(x_test)):\n",
    "#             max_sim = 0\n",
    "#             max_index = 0\n",
    "#             for j in range(self.x_train.shape[0]):\n",
    "#                 temp = self.document_similarity(x_test[i], self.x_train[j])\n",
    "#                 if temp > max_sim:\n",
    "#                     max_sim = temp\n",
    "#                     max_index = j\n",
    "#                     print(f\"Similar sentence: {self.x_train[j]}\")\n",
    "#             y_predict.append(self.y_train[max_index])\n",
    "#         return y_predict\n",
    "    \n",
    "#     def convert_tag(self, tag):\n",
    "#         \"\"\"Convert the tag given by nltk.pos_tag to the tag used by wordnet.synsets\"\"\"\n",
    "#         tag_dict = {'N': 'n', 'J': 'a', 'R': 'r', 'V': 'v'}\n",
    "#         try:\n",
    "#             return tag_dict[tag[0]]\n",
    "#         except KeyError:\n",
    "#             return None\n",
    "\n",
    "#     def doc_to_synsets(self, doc):\n",
    "#         \"\"\"\n",
    "#             Returns a list of synsets in document.\n",
    "#             Tokenizes and tags the words in the document doc.\n",
    "#             Then finds the first synset for each word/tag combination.\n",
    "#         If a synset is not found for that combination it is skipped.\n",
    "\n",
    "#         Args:\n",
    "#             doc: string to be converted\n",
    "\n",
    "#         Returns:\n",
    "#             list of synsets\n",
    "#         \"\"\"\n",
    "#         tokens = word_tokenize(doc+' ')\n",
    "        \n",
    "#         l = []\n",
    "#         tags = nltk.pos_tag([tokens[0] + ' ']) if len(tokens) == 1 else nltk.pos_tag(tokens)\n",
    "        \n",
    "#         for token, tag in zip(tokens, tags):\n",
    "#             syntag = self.convert_tag(tag[1])\n",
    "#             syns = wn.synsets(token, syntag)\n",
    "#             if (len(syns) > 0):\n",
    "#                 l.append(syns[0])\n",
    "#         return l  \n",
    "    \n",
    "#     def similarity_score(self, s1, s2, distance_type = 'path'):\n",
    "#         \"\"\"\n",
    "#         Calculate the normalized similarity score of s1 onto s2\n",
    "#         For each synset in s1, finds the synset in s2 with the largest similarity value.\n",
    "#         Sum of all of the largest similarity values and normalize this value by dividing it by the\n",
    "#         number of largest similarity values found.\n",
    "\n",
    "#         Args:\n",
    "#           s1, s2: list of synsets from doc_to_synsets\n",
    "\n",
    "#         Returns:\n",
    "#           normalized similarity score of s1 onto s2\n",
    "#         \"\"\"\n",
    "#         s1_largest_scores = []\n",
    "\n",
    "#         for i, s1_synset in enumerate(s1, 0):\n",
    "#             max_score = 0\n",
    "#             for s2_synset in s2:\n",
    "#                 if distance_type == 'path':\n",
    "#                     score = s1_synset.path_similarity(s2_synset, simulate_root = False)\n",
    "#                 else:\n",
    "#                     score = s1_synset.wup_similarity(s2_synset)                  \n",
    "#                 if score != None:\n",
    "#                     if score > max_score:\n",
    "#                         max_score = score\n",
    "\n",
    "#             if max_score != 0:\n",
    "#                 s1_largest_scores.append(max_score)\n",
    "\n",
    "#         mean_score = np.mean(s1_largest_scores)\n",
    "\n",
    "#         return mean_score \n",
    "    \n",
    "#     def document_similarity(self,doc1, doc2):\n",
    "#         \"\"\"Finds the symmetrical similarity between doc1 and doc2\"\"\"\n",
    "\n",
    "#         synsets1 = self.doc_to_synsets(doc1)\n",
    "#         synsets2 = self.doc_to_synsets(doc2)\n",
    "          \n",
    "#         return (self.similarity_score(synsets1, synsets2) + self.similarity_score(synsets2, synsets1)) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba3851da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc1 = 'I like rains'\n",
    "# doc2 = 'I like showers'\n",
    "# x = KNN_NLC_Classifer(k=1)\n",
    "# print(\"Test Similarity Score: \", x.document_similarity(doc1, doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1b29755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format tags columns in df\n",
    "\n",
    "def format_tags(df):\n",
    "    tags = []\n",
    "    tag_lists = []\n",
    "\n",
    "    for subjects in df.subjects:\n",
    "        if type(subjects) is str:\n",
    "            s = subjects.split(\",\")\n",
    "        else:\n",
    "            if type(subjects) is list:\n",
    "                s = subjects\n",
    "            else:\n",
    "                s = []\n",
    "        s = [t.lstrip().rstrip() for t in s]\n",
    "        tag_lists.append(s)\n",
    "        for tag in s:\n",
    "            tags.append(tag)\n",
    "    df['tags'] = tag_lists\n",
    "    return df, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b376ef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "health_tags = ['Health', 'Health News', \"Health Care\", 'Medical', 'Public Health']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a872b84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def health(x):\n",
    "    for t in health_tags:\n",
    "        if t in x:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae45172e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   claim_id                                               text  \\\n",
      "0      8713  Britain to reveal trial criteria for coronavir...   \n",
      "1      2768  U.S. says results encouraging for healthcare d...   \n",
      "2      2717  Latest trial in J&J talc litigations gets unde...   \n",
      "3      5793  Democrats hoping to flip House not just trash-...   \n",
      "4      2981  Sex tech from women-led startups pops up at CE...   \n",
      "5     29528                         Waxed apples cause cancer.   \n",
      "6     15232  Rhode Island will become just the second state...   \n",
      "7      7453  Brazil cities lurch to lockdowns amid virus cr...   \n",
      "8      8069  Slovakia's new government to sharply ramp up c...   \n",
      "9     26723       The coronavirus is “simply the common cold.”   \n",
      "10     5243  Massachusetts to help test addiction treatment...   \n",
      "11     4639  ALS patient behind ice bucket challenge: I wil...   \n",
      "12     3801  State Senate leader outlines agenda as lawmake...   \n",
      "13    14336  \"Rick Scott's Starbucks heckler Says Rick Scot...   \n",
      "14     8737  'Enemy of mankind': Coronavirus deaths top SAR...   \n",
      "\n",
      "      date_published                                        explanation  \\\n",
      "0      April 7, 2020  British regulators will this week reveal appro...   \n",
      "1   January 30, 2014  The Obama administration on Thursday reported ...   \n",
      "2    January 8, 2019  A California jury on Monday heard opening stat...   \n",
      "3                NaN  Democrats hoping to flip enough seats to regai...   \n",
      "4                NaN  Sex tech is gracing the CES gadget show in Las...   \n",
      "5        May 3, 2016  \"What's true: Apples are commonly coated with ...   \n",
      "6    August 23, 2015  \"The center says \"\"Rhode Island will become ju...   \n",
      "7                NaN  Faced with overwhelmed hospitals and surging c...   \n",
      "8     March 26, 2020  Slovakia aims to sharply increase daily corona...   \n",
      "9      March 9, 2020  The 2019 coronavirus is part of a family of vi...   \n",
      "10               NaN  Massachusetts has agreed to participate in a n...   \n",
      "11               NaN  Pete Frates was mistakenly written off as dead...   \n",
      "12               NaN  The Democratic leader of the Massachusetts Sen...   \n",
      "13     April 6, 2016  \"The activist at Starbucks said Rick Scott \"\"c...   \n",
      "14  February 8, 2020  China raised the death toll from its coronavir...   \n",
      "\n",
      "                          fact_checkers  \\\n",
      "0                        Alistair Smout   \n",
      "1                          David Morgan   \n",
      "2                           Tina Bellon   \n",
      "3                         Steve Leblanc   \n",
      "4                         Rachel Lerman   \n",
      "5                          Kim LaCapria   \n",
      "6                         Mark Reynolds   \n",
      "7    David Biller And Mauricio Savarese   \n",
      "8                                         \n",
      "9                         Tom Kertscher   \n",
      "10                                        \n",
      "11                       Philip Marcelo   \n",
      "12                         Bob Salsberg   \n",
      "13                        Joshua Gillin   \n",
      "14         Winni Zhou, Dominique Patton   \n",
      "\n",
      "                                            main_text  \\\n",
      "0   Antibody tests show whether whether people hav...   \n",
      "1   As part of President Barack Obama’s healthcare...   \n",
      "2   The lawsuit brought by Terry Leavitt in Alamed...   \n",
      "3   Those candidates include Lauren Underwood, a 3...   \n",
      "4   CES is allowing space for sex tech companies a...   \n",
      "5   In April 2016, the web site MetaSpoon publishe...   \n",
      "6   \"Rhode Island has issued a controversial vacci...   \n",
      "7   The movements of Brazilians have been complete...   \n",
      "8   Prime Minister Igor Matovic, whose team took o...   \n",
      "9   \"If the current coronavirus were really the sa...   \n",
      "10  State health officials say the rating system w...   \n",
      "11  Frates is fighting back from a summer health s...   \n",
      "12  Republican Gov. Charlie Baker administered oat...   \n",
      "13  \"It starts like a joke — Gov. Rick Scott walks...   \n",
      "14  Many of China’s usually teeming cities have al...   \n",
      "\n",
      "                                              sources label_categorical  \\\n",
      "0                                                                  true   \n",
      "1                                                                  true   \n",
      "2   uk.reuters.com/companies/IMTP.PA,uk.reuters.co...              true   \n",
      "3                                                                  true   \n",
      "4   /13ff3df5aa6b428db431a7f50c4b7c5d,/cf15491db21...              true   \n",
      "5                                                                 false   \n",
      "6   http://www.providencejournal.com/article/20150...           mixture   \n",
      "7                                                                  true   \n",
      "8                                                                  true   \n",
      "9   https://www.cdc.gov/coronavirus/2019-ncov/abou...             false   \n",
      "10                https://www.shatterproof.org/rating              true   \n",
      "11  http://www.wral.com/fda-approves-first-new-dru...              true   \n",
      "12                                                                 true   \n",
      "13  http://www.gainesville.com/article/20160405/AR...           mixture   \n",
      "14  /article/us-china-health-france/in-french-alpi...              true   \n",
      "\n",
      "                                             subjects  \\\n",
      "0                                         Health News   \n",
      "1                                         Health News   \n",
      "2                                         Health News   \n",
      "3   Access to health care, Health, Politics, North...   \n",
      "4   Technology, Consumer Electronics Show, General...   \n",
      "5           Medical, apples, food warnings, metaspoon   \n",
      "6   Rhode Island, Health Care, Government Regulati...   \n",
      "7   Brazil, Rio de Janeiro, AP Top News, Health, G...   \n",
      "8                                         Health News   \n",
      "9   Public Health, Facebook Fact-checks, Coronavir...   \n",
      "10         Health, Massachusetts, Addiction treatment   \n",
      "11  Pete Frates, Health, Boston, Lou Gehrigs disea...   \n",
      "12  Legislature, Mental health, Prescription drug ...   \n",
      "13  Health Care, Medicaid, Florida, Rick Scott's S...   \n",
      "14                                        Health News   \n",
      "\n",
      "                                                 tags  label  \\\n",
      "0                                       [Health News]      0   \n",
      "1                                       [Health News]      0   \n",
      "2                                       [Health News]      0   \n",
      "3   [Access to health care, Health, Politics, Nort...      0   \n",
      "4   [Technology, Consumer Electronics Show, Genera...      0   \n",
      "5         [Medical, apples, food warnings, metaspoon]      1   \n",
      "6   [Rhode Island, Health Care, Government Regulat...      2   \n",
      "7   [Brazil, Rio de Janeiro, AP Top News, Health, ...      0   \n",
      "8                                       [Health News]      0   \n",
      "9   [Public Health, Facebook Fact-checks, Coronavi...      1   \n",
      "10       [Health, Massachusetts, Addiction treatment]      0   \n",
      "11  [Pete Frates, Health, Boston, Lou Gehrigs dise...      0   \n",
      "12  [Legislature, Mental health, Prescription drug...      0   \n",
      "13  [Health Care, Medicaid, Florida, Rick Scott's ...      2   \n",
      "14                                      [Health News]      0   \n",
      "\n",
      "                                        text_original  \n",
      "0   Britain to reveal trial criteria for coronavir...  \n",
      "1   U.S. says results encouraging for healthcare d...  \n",
      "2   Latest trial in J&J talc litigations gets unde...  \n",
      "3   Democrats hoping to flip House not just trash-...  \n",
      "4   Sex tech from women-led startups pops up at CE...  \n",
      "5                          Waxed apples cause cancer.  \n",
      "6   Rhode Island will become just the second state...  \n",
      "7   Brazil cities lurch to lockdowns amid virus cr...  \n",
      "8   Slovakia's new government to sharply ramp up c...  \n",
      "9        The coronavirus is “simply the common cold.”  \n",
      "10  Massachusetts to help test addiction treatment...  \n",
      "11  ALS patient behind ice bucket challenge: I wil...  \n",
      "12  State Senate leader outlines agenda as lawmake...  \n",
      "13  \"Rick Scott's Starbucks heckler Says Rick Scot...  \n",
      "14  'Enemy of mankind': Coronavirus deaths top SAR...  \n",
      "\n",
      "Size of input file is  (3596, 12)\n"
     ]
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "FILENAME = \"../PUBHEALTH/train.tsv\"          \n",
    "\n",
    "dataset = pd.read_csv(FILENAME, sep='\\t')\n",
    "dataset, dataset_tags = format_tags(dataset)\n",
    "\n",
    "mask = dataset['tags'].apply(lambda x: health(x))\n",
    "dataset = dataset[mask]\n",
    "\n",
    "text_col = \"text\"\n",
    "answer_col = \"label\"\n",
    "\n",
    "dataset.rename(columns = {\"claim\": \"text\", \"label\":\"label_categorical\"}, inplace = True)\n",
    "dataset[\"label\"] = pd.factorize(dataset[\"label_categorical\"])[0]\n",
    "dataset = dataset.dropna(subset=[text_col])\n",
    "dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Make a copy of the 'text' column\n",
    "dataset['text_original'] = dataset['text']\n",
    "\n",
    "Num_Words = dataset.shape[0]\n",
    "\n",
    "print(dataset.head(15))\n",
    "print(\"\\nSize of input file is \", dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3350a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nataliewang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is the sample of training text after removing the stop words\n",
      "0    britain reveal trial criterion coronavirus ant...\n",
      "1    u.s. say result encouraging healthcare deliver...\n",
      "2    latest trial j&j talc litigation get way calif...\n",
      "3      democrat hoping flip house trash-talking trump.\n",
      "4       sex tech women-led startup pop ce gadget show.\n",
      "5                            waxed apple cause cancer.\n",
      "6    rhode island become second state mandate vacci...\n",
      "7    brazil city lurch lockdown amid virus crisis r...\n",
      "8    slovakia's new government sharply ramp coronav...\n",
      "9                    coronavirus “simply common cold.”\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "nltk.download('stopwords')\n",
    "s = stopwords.words('english')\n",
    "#add additional stop words\n",
    "s.extend(['today', 'tomorrow', 'outside', 'out', 'there'])\n",
    "ps = nltk.wordnet.WordNetLemmatizer()\n",
    "for i in range(dataset.shape[0]):\n",
    "    review = dataset.loc[i,'text']\n",
    "#     review = re.sub('[^a-zA-Z]', ' ', dataset.loc[i,'text'])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [ps.lemmatize(word) for word in review if not word in s]\n",
    "    review = ' '.join(review)\n",
    "    dataset.loc[i, 'text'] = review\n",
    "    X_train = dataset['text']\n",
    "y_train = dataset['label']\n",
    "print(\"Below is the sample of training text after removing the stop words\")\n",
    "print(dataset['text'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4106218",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # 4. Train the Classifier\n",
    "# classifier = KNN_NLC_Classifer(k=1, distance_type='path')\n",
    "# classifier.fit(X_train, y_train)\n",
    "\n",
    "# final_test_list = ['Cranberries help UTIs.']\n",
    "\n",
    "# test_corpus = []\n",
    "# for i in range(len(final_test_list)):\n",
    "#     review = re.sub('[^a-zA-Z]', ' ', final_test_list[i])\n",
    "#     review = review.lower()\n",
    "#     review = review.split()\n",
    "\n",
    "#     review = [ps.lemmatize(word) for word in review if not word in s]\n",
    "#     review = ' '.join(review)\n",
    "#     test_corpus.append(review)\n",
    "\n",
    "# print(\"predicting\")\n",
    "# y_pred_final = classifier.predict(test_corpus)\n",
    "\n",
    "# output_df = pd.DataFrame(data = {'text': final_test_list, 'numerical_pred_label': y_pred_final})\n",
    "# # output_df['answer'] = np.where(output_df['code']==1, 'Temperature','Conditions')\n",
    "# print(output_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f20db59",
   "metadata": {},
   "source": [
    "# K=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7453eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_NLC_Classifer():\n",
    "    def __init__(self, k=3, distance_type = 'path'):\n",
    "        self.k = k\n",
    "        self.distance_type = distance_type\n",
    "\n",
    "    # This function is used for training\n",
    "    def fit(self, x_train, y_train):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    # This function runs the K(n) nearest neighbour algorithm and\n",
    "    # returns the label with closest match. \n",
    "    # Predict returns the n similar sentences as a list of tuples [(sentence, score), (sentence, score), ...]\n",
    "    def predict(self, x_test):\n",
    "        self.x_test = x_test\n",
    "        y_predict = []\n",
    "        \n",
    "        score_to_text_dict = {}\n",
    "        score_to_index_dict = {}\n",
    "\n",
    "        for i in range(len(x_test)):\n",
    "            for j in range(self.x_train.shape[0]): # Index j of the sentences contains the current sentence\n",
    "                temp = self.document_similarity(x_test[i], self.x_train[j])\n",
    "                score_to_text_dict[temp] = self.x_train[j]  \n",
    "                score_to_index_dict[temp] = j\n",
    "                \n",
    "        similar_texts_list = []\n",
    "        \n",
    "        sorted_scores = list(score_to_text_dict.keys())\n",
    "        sorted_scores.sort(reverse=True)\n",
    "        \n",
    "        for i in range(self.k):\n",
    "            score = sorted_scores[i]\n",
    "            new_tuple = (score_to_text_dict[score], score_to_index_dict[score], score)\n",
    "            similar_texts_list.append(new_tuple)\n",
    "\n",
    "        return similar_texts_list\n",
    "    \n",
    "    def convert_tag(self, tag):\n",
    "        \"\"\"Convert the tag given by nltk.pos_tag to the tag used by wordnet.synsets\"\"\"\n",
    "        tag_dict = {'N': 'n', 'J': 'a', 'R': 'r', 'V': 'v'}\n",
    "        try:\n",
    "            return tag_dict[tag[0]]\n",
    "        except KeyError:\n",
    "            return None\n",
    "\n",
    "    def doc_to_synsets(self, doc):\n",
    "        \"\"\"\n",
    "            Returns a list of synsets in document.\n",
    "            Tokenizes and tags the words in the document doc.\n",
    "            Then finds the first synset for each word/tag combination.\n",
    "        If a synset is not found for that combination it is skipped.\n",
    "\n",
    "        Args:\n",
    "            doc: string to be converted\n",
    "\n",
    "        Returns:\n",
    "            list of synsets\n",
    "        \"\"\"\n",
    "        tokens = word_tokenize(doc+' ')\n",
    "        \n",
    "        l = []\n",
    "        tags = nltk.pos_tag([tokens[0] + ' ']) if len(tokens) == 1 else nltk.pos_tag(tokens)\n",
    "        \n",
    "        for token, tag in zip(tokens, tags):\n",
    "            syntag = self.convert_tag(tag[1])\n",
    "            syns = wn.synsets(token, syntag)\n",
    "            if (len(syns) > 0):\n",
    "                l.append(syns[0])\n",
    "        return l  \n",
    "    \n",
    "    def similarity_score(self, s1, s2, distance_type = 'path'):\n",
    "        \"\"\"\n",
    "        Calculate the normalized similarity score of s1 onto s2\n",
    "        For each synset in s1, finds the synset in s2 with the largest similarity value.\n",
    "        Sum of all of the largest similarity values and normalize this value by dividing it by the\n",
    "        number of largest similarity values found.\n",
    "\n",
    "        Args:\n",
    "          s1, s2: list of synsets from doc_to_synsets\n",
    "\n",
    "        Returns:\n",
    "          normalized similarity score of s1 onto s2\n",
    "        \"\"\"\n",
    "        s1_largest_scores = []\n",
    "\n",
    "        for i, s1_synset in enumerate(s1, 0):\n",
    "            max_score = 0\n",
    "            for s2_synset in s2:\n",
    "                if distance_type == 'path':\n",
    "                    score = s1_synset.path_similarity(s2_synset, simulate_root = False)\n",
    "                else:\n",
    "                    score = s1_synset.wup_similarity(s2_synset)                  \n",
    "                if score != None:\n",
    "                    if score > max_score:\n",
    "                        max_score = score\n",
    "\n",
    "            if max_score != 0:\n",
    "                s1_largest_scores.append(max_score)\n",
    "\n",
    "        mean_score = np.mean(s1_largest_scores)\n",
    "\n",
    "        return mean_score \n",
    "    \n",
    "    def document_similarity(self,doc1, doc2):\n",
    "        \"\"\"Finds the symmetrical similarity between doc1 and doc2\"\"\"\n",
    "\n",
    "        synsets1 = self.doc_to_synsets(doc1)\n",
    "        synsets2 = self.doc_to_synsets(doc2)\n",
    "          \n",
    "        return (self.similarity_score(synsets1, synsets2) + self.similarity_score(synsets2, synsets1)) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd8d8887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['claim_id', 'text', 'date_published', 'explanation', 'fact_checkers',\n",
       "       'main_text', 'sources', 'label_categorical', 'subjects', 'tags',\n",
       "       'label', 'text_original'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad9d1e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting...\n",
      "Top 3 similar examples:\n",
      "\"mixing cream tartar orange juice flush nicotine body help quit smoking faster.\", index: 3105, score: 0.4090097402597403\n",
      "Original sentence: Mixing cream of tartar with orange juice will flush nicotine from your body and help you quit smoking faster. \n",
      "Label: false\n",
      "\n",
      "\"trans teen’s war body started 10.\", index: 2046, score: 0.30969065656565653\n",
      "Original sentence: Trans teen’s war with his body started when he was just 10.\n",
      "Label: true\n",
      "\n",
      "\"uk cost body finally approves limited use gsk's lupus drug.\", index: 2824, score: 0.30832702020202024\n",
      "Original sentence: UK cost body finally approves limited use of GSK's lupus drug.\n",
      "Label: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Train the Classifier\n",
    "classifier = KNN_NLC_Classifer(k=3, distance_type='path')\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "final_test_list = ['My hypothesis is that you should train your body how to properly and quickly flush itself of lactic acid without supplements.']\n",
    "\n",
    "test_corpus = []\n",
    "for i in range(len(final_test_list)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', final_test_list[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "\n",
    "    review = [ps.lemmatize(word) for word in review if not word in s]\n",
    "    review = ' '.join(review)\n",
    "    test_corpus.append(review)\n",
    "\n",
    "print(\"Predicting...\")\n",
    "y_pred_final = classifier.predict(test_corpus)\n",
    "print(\"Top 3 similar examples:\")\n",
    "\n",
    "for sentence in y_pred_final:\n",
    "    print(f\"\\\"{sentence[0]}\\\", index: {sentence[1]}, score: {sentence[2]}\")\n",
    "    row_data = dataset.iloc[[sentence[1]]].values.tolist()[0]\n",
    "    text_original_index = 11\n",
    "    label_categorical_index = 7\n",
    "    print(f'Original sentence: {row_data[text_original_index]}')\n",
    "    print(f'Label: {row_data[label_categorical_index]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8668377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
