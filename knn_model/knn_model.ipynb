{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4bbd4be",
   "metadata": {},
   "source": [
    "# Reddit HealthAdviceChecKer Bot Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c812088",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb34f8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package genesis to\n",
      "[nltk_data]     /Users/gwenythportillowightman/nltk_data...\n",
      "[nltk_data]   Package genesis is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gwenythportillowightman/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/gwenythportillowightman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/gwenythportillowightman/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import genesis\n",
    "nltk.download('genesis')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "genesis_ic = wn.ic(genesis, False, 0.0)\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667ae606",
   "metadata": {},
   "source": [
    "## Filtering the dataset to health claims only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1b29755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format tags columns in df\n",
    "\n",
    "def format_tags(df):\n",
    "    tags = []\n",
    "    tag_lists = []\n",
    "\n",
    "    for subjects in df.subjects:\n",
    "        if type(subjects) is str:\n",
    "            s = subjects.split(\",\")\n",
    "        else:\n",
    "            if type(subjects) is list:\n",
    "                s = subjects\n",
    "            else:\n",
    "                s = []\n",
    "        s = [t.lstrip().rstrip() for t in s]\n",
    "        tag_lists.append(s)\n",
    "        for tag in s:\n",
    "            tags.append(tag)\n",
    "    df['tags'] = tag_lists\n",
    "    return df, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b376ef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select claims from relevant categories\n",
    "health_tags = ['Health', 'Health News', \"Health Care\", 'Medical', 'Public Health', 'ADHD', 'Health / Medical', 'Medical Myths', 'diet']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a872b84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for masking dataframe with relevant tags\n",
    "def health(x):\n",
    "    for t in health_tags:\n",
    "        if t in x:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724fd1f0",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae45172e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df (3660, 12)\n",
      "     claim_id                                               text  \\\n",
      "3637    38118   The FDA published “conclusive proof” that the...   \n",
      "3638      NaN  There have been cases of imported fruits and v...   \n",
      "3639      NaN  A 9 year old in Montana died from lead poisoni...   \n",
      "3640      NaN  Be sure to wash your fruits and vegetables fro...   \n",
      "3641      NaN  The lead in candles helps you reach your daily...   \n",
      "3642      NaN  Apple cider vinegar mixed in water speeds up y...   \n",
      "3643      NaN  Apple cider vinegar is a great way to lose wei...   \n",
      "3644      NaN  Dryer sheets are full of toxic chemicals. Keep...   \n",
      "3645      NaN  Using a dryer sheet in your laundry gives you ...   \n",
      "3646      NaN  A spoonful of safflower oil a day keeps the po...   \n",
      "3647      NaN  When I was trying to lose weight, I only used ...   \n",
      "3648      NaN  Delayed-release ADHD medication provides all-d...   \n",
      "3649      NaN  Extended-release Adderall reaches maximum bloo...   \n",
      "3650      NaN  Vaping doesn't cause any lung problems, it act...   \n",
      "3651      NaN              You cannot die due to e-cigarette use   \n",
      "3652      NaN  My grandmother drank cold water after she ate ...   \n",
      "3653      NaN  Cold water slows down your digestion so you sh...   \n",
      "3654      NaN  Sunscreen actually makes you more wrinkled whi...   \n",
      "3655      NaN  Sun exposure actually makes you more resistant...   \n",
      "3656      NaN  Many people report using marijuana to cope wit...   \n",
      "3657      NaN  While studies conducted specifically on social...   \n",
      "3658      NaN  Algae is just as healthy as any vegetable you ...   \n",
      "3659      NaN  Drinking water from rivers and streams are alw...   \n",
      "\n",
      "     date_published                                        explanation  \\\n",
      "3637      22-Nov-17  FDA Confirms DTaP Vaccine Causes Autism in Nov...   \n",
      "3638            NaN                                                NaN   \n",
      "3639            NaN                                                NaN   \n",
      "3640            NaN                                                NaN   \n",
      "3641            NaN                                                NaN   \n",
      "3642            NaN                                                NaN   \n",
      "3643            NaN                                                NaN   \n",
      "3644            NaN                                                NaN   \n",
      "3645            NaN                                                NaN   \n",
      "3646            NaN                                                NaN   \n",
      "3647            NaN                                                NaN   \n",
      "3648            NaN                                                NaN   \n",
      "3649            NaN                                                NaN   \n",
      "3650            NaN                                                NaN   \n",
      "3651            NaN                                                NaN   \n",
      "3652            NaN                                                NaN   \n",
      "3653            NaN                                                NaN   \n",
      "3654            NaN                                                NaN   \n",
      "3655            NaN                                                NaN   \n",
      "3656            NaN                                                NaN   \n",
      "3657            NaN                                                NaN   \n",
      "3658            NaN                                                NaN   \n",
      "3659            NaN                                                NaN   \n",
      "\n",
      "                fact_checkers  \\\n",
      "3637    Rich Buhler & Staff     \n",
      "3638                      NaN   \n",
      "3639                      NaN   \n",
      "3640                      NaN   \n",
      "3641                      NaN   \n",
      "3642                      NaN   \n",
      "3643                      NaN   \n",
      "3644                      NaN   \n",
      "3645                      NaN   \n",
      "3646                      NaN   \n",
      "3647                      NaN   \n",
      "3648                      NaN   \n",
      "3649                      NaN   \n",
      "3650                      NaN   \n",
      "3651                      NaN   \n",
      "3652                      NaN   \n",
      "3653                      NaN   \n",
      "3654                      NaN   \n",
      "3655                      NaN   \n",
      "3656                      NaN   \n",
      "3657                      NaN   \n",
      "3658                      NaN   \n",
      "3659                      NaN   \n",
      "\n",
      "                                              main_text  \\\n",
      "3637  The FDA hasn’t confirmed a link between DTaP v...   \n",
      "3638                                                NaN   \n",
      "3639                                                NaN   \n",
      "3640                                                NaN   \n",
      "3641                                                NaN   \n",
      "3642                                                NaN   \n",
      "3643                                                NaN   \n",
      "3644                                                NaN   \n",
      "3645                                                NaN   \n",
      "3646                                                NaN   \n",
      "3647                                                NaN   \n",
      "3648                                                NaN   \n",
      "3649                                                NaN   \n",
      "3650                                                NaN   \n",
      "3651                                                NaN   \n",
      "3652                                                NaN   \n",
      "3653                                                NaN   \n",
      "3654                                                NaN   \n",
      "3655                                                NaN   \n",
      "3656                                                NaN   \n",
      "3657                                                NaN   \n",
      "3658                                                NaN   \n",
      "3659                                                NaN   \n",
      "\n",
      "                                                sources label_categorical  \\\n",
      "3637  https://www.truthorfiction.com/marshall-kamena...             FALSE   \n",
      "3638                                                NaN             FALSE   \n",
      "3639                                                NaN              TRUE   \n",
      "3640                                                NaN             FALSE   \n",
      "3641                                                NaN             FALSE   \n",
      "3642                                                NaN             FALSE   \n",
      "3643                                                NaN             FALSE   \n",
      "3644                                                NaN             FALSE   \n",
      "3645                                                NaN             FALSE   \n",
      "3646                                                NaN             FALSE   \n",
      "3647                                                NaN             FALSE   \n",
      "3648                                                NaN              TRUE   \n",
      "3649                                                NaN              TRUE   \n",
      "3650                                                NaN             FALSE   \n",
      "3651                                                NaN             FALSE   \n",
      "3652                                                NaN             FALSE   \n",
      "3653                                                NaN             FALSE   \n",
      "3654                                                NaN             FALSE   \n",
      "3655                                                NaN             FALSE   \n",
      "3656                                                NaN              TRUE   \n",
      "3657                                                NaN              TRUE   \n",
      "3658                                                NaN             FALSE   \n",
      "3659                                                NaN             FALSE   \n",
      "\n",
      "     subjects       tags  label  \\\n",
      "3637  Medical  [Medical]      1   \n",
      "3638   Health   [Health]      1   \n",
      "3639   Health   [Health]      0   \n",
      "3640   Health   [Health]      1   \n",
      "3641   Health   [Health]      1   \n",
      "3642   Health   [Health]      1   \n",
      "3643   Health   [Health]      1   \n",
      "3644   Health   [Health]      1   \n",
      "3645   Health   [Health]      1   \n",
      "3646   Health   [Health]      1   \n",
      "3647   Health   [Health]      1   \n",
      "3648   Health   [Health]      0   \n",
      "3649   Health   [Health]      0   \n",
      "3650   Health   [Health]      1   \n",
      "3651   Health   [Health]      1   \n",
      "3652   Health   [Health]      1   \n",
      "3653   Health   [Health]      1   \n",
      "3654   Health   [Health]      1   \n",
      "3655   Health   [Health]      1   \n",
      "3656   Health   [Health]      0   \n",
      "3657   Health   [Health]      0   \n",
      "3658   Health   [Health]      1   \n",
      "3659   Health   [Health]      1   \n",
      "\n",
      "                                          text_original  \n",
      "3637   The FDA published “conclusive proof” that the...  \n",
      "3638  There have been cases of imported fruits and v...  \n",
      "3639  A 9 year old in Montana died from lead poisoni...  \n",
      "3640  Be sure to wash your fruits and vegetables fro...  \n",
      "3641  The lead in candles helps you reach your daily...  \n",
      "3642  Apple cider vinegar mixed in water speeds up y...  \n",
      "3643  Apple cider vinegar is a great way to lose wei...  \n",
      "3644  Dryer sheets are full of toxic chemicals. Keep...  \n",
      "3645  Using a dryer sheet in your laundry gives you ...  \n",
      "3646  A spoonful of safflower oil a day keeps the po...  \n",
      "3647  When I was trying to lose weight, I only used ...  \n",
      "3648  Delayed-release ADHD medication provides all-d...  \n",
      "3649  Extended-release Adderall reaches maximum bloo...  \n",
      "3650  Vaping doesn't cause any lung problems, it act...  \n",
      "3651              You cannot die due to e-cigarette use  \n",
      "3652  My grandmother drank cold water after she ate ...  \n",
      "3653  Cold water slows down your digestion so you sh...  \n",
      "3654  Sunscreen actually makes you more wrinkled whi...  \n",
      "3655  Sun exposure actually makes you more resistant...  \n",
      "3656  Many people report using marijuana to cope wit...  \n",
      "3657  While studies conducted specifically on social...  \n",
      "3658  Algae is just as healthy as any vegetable you ...  \n",
      "3659  Drinking water from rivers and streams are alw...  \n"
     ]
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "data_file_path = \"train.tsv\"          \n",
    "\n",
    "df = pd.read_csv(data_file_path, sep='\\t')\n",
    "df, df_tags = format_tags(df)\n",
    "\n",
    "mask = df['tags'].apply(lambda x: health(x))\n",
    "df = df[mask]\n",
    "\n",
    "# text_col contains the column name of where claims are found\n",
    "# answer_col contains the column name of where post labels (true, false, etc.) are found\n",
    "text_col = \"text\"\n",
    "answer_col = \"label\"\n",
    "\n",
    "# Rename the claim column to \"text\" and label column to \"label_categorical\"\n",
    "df.rename(columns = {\"claim\": \"text\", \"label\": \"label_categorical\"}, inplace = True)\n",
    "# Make the categorical labels into numbers (0, 1, 2, 3)\n",
    "df[\"label\"] = pd.factorize(df[\"label_categorical\"])[0]\n",
    "df = df.dropna(subset=[text_col])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Make a copy of the 'text' column\n",
    "df['text_original'] = df['text']\n",
    "\n",
    "print(f\"Shape of df {df.shape}\")\n",
    "print(df[-23:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5e170e",
   "metadata": {},
   "source": [
    "### Prepare to preprocess text claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c281cfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gwenythportillowightman/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "nltk.download('stopwords')\n",
    "s = stopwords.words('english')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    text = [ps.lemmatize(word) for word in text if not word in s]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603302cb",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3350a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed claims\n",
      "0    britain reveal trial criterion coronavirus ant...\n",
      "1    u say result encouraging healthcare delivery r...\n",
      "2    latest trial j j talc litigation get way calif...\n",
      "3       democrat hoping flip house trash talking trump\n",
      "4        sex tech woman led startup pop ce gadget show\n",
      "5                             waxed apple cause cancer\n",
      "6    rhode island become second state mandate vacci...\n",
      "7    brazil city lurch lockdown amid virus crisis r...\n",
      "8    slovakia new government sharply ramp coronavir...\n",
      "9                       coronavirus simply common cold\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "ps = nltk.wordnet.WordNetLemmatizer()\n",
    "for i in range(df.shape[0]):\n",
    "    text = df.loc[i,'text']\n",
    "    text = preprocess_text(text)\n",
    "    df.loc[i, 'text'] = text\n",
    "    X_train = df['text']\n",
    "y_train = df['label']\n",
    "\n",
    "print(\"Preprocessed claims\")\n",
    "print(df['text'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa371b8",
   "metadata": {},
   "source": [
    "## KNN Model\n",
    "\n",
    "Returns the top k most similar sentences from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7453eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_Model():\n",
    "    def __init__(self, k=3, distance_type = 'path', preprocess=True):\n",
    "        self.k = k\n",
    "        self.distance_type = distance_type\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    # This function is used for training\n",
    "    def fit(self, x_train, y_train):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def split_input(self, input_sentence):\n",
    "        test_corpus = []\n",
    "        \n",
    "        # Preprocess the full x_test input\n",
    "        input_sentence_copy = copy.deepcopy(input_sentence)\n",
    "        if self.preprocess:\n",
    "            input_sentence_copy = preprocess_text(input_sentence_copy)\n",
    "        \n",
    "        # Preprocess sentences of the input\n",
    "        sentences = sent_tokenize(input_sentence)\n",
    "        for sentence in sentences:\n",
    "            if self.preprocess:\n",
    "                sentence = preprocess_text(sentence)\n",
    "            test_corpus.append(sentence)\n",
    "            \n",
    "        if len(test_corpus) > 1:\n",
    "            test_corpus.append(input_sentence_copy)\n",
    "        \n",
    "        return test_corpus\n",
    "\n",
    "    # Returns the k most similar sentences for the input sentence\n",
    "    # Predict returns the n similar sentences as a list of tuples [(sentence, score), (sentence, score), ...]\n",
    "    # Takes in only one input at a time\n",
    "    def predict(self, x_test):\n",
    "        test_corpus = self.split_input(x_test)\n",
    "            \n",
    "        self.x_test = test_corpus\n",
    "    \n",
    "        # {score: [(index of sentence in `test_corpus`, similar sentence index in `dataset`)], ...}\n",
    "        all_top_scores_dict = {}\n",
    "\n",
    "        # Iterate over sentences of the input\n",
    "        for i in range(len(self.x_test)):\n",
    "            print(f\"------- Getting similar sentences for \\\"{self.x_test[i]}\\\" ({i+1}/{len(self.x_test)}) ------\")\n",
    "            \n",
    "            # {score: similar_sentence_index_in_`dataset`, ...}\n",
    "            score_to_index_dict = {}\n",
    "            \n",
    "            # Iterate over training examples and find sentence similarity scores\n",
    "            for j in range(self.x_train.shape[0]): \n",
    "                score = self.document_similarity(self.x_test[i], self.x_train[j])\n",
    "                score_to_index_dict[score] = j\n",
    "\n",
    "            sorted_scores = list(score_to_index_dict.keys())\n",
    "            sorted_scores.sort(reverse=True)\n",
    "\n",
    "            # Get the top k similar sentences for the current sentence (x_test[i])\n",
    "            for k in range(self.k):\n",
    "                score = sorted_scores[k]\n",
    "                \n",
    "                if score in all_top_scores_dict:\n",
    "                    all_top_scores_dict[score].append( (i, score_to_index_dict[score]) )\n",
    "                else:\n",
    "                    all_top_scores_dict[score] = [ (i, score_to_index_dict[score]) ]\n",
    "                    \n",
    "        # Get the top k scoring sentences and similar sentences from all_top_scores_dict\n",
    "        sorted_scores = list(all_top_scores_dict.keys())\n",
    "        sorted_scores.sort(reverse=True)\n",
    "        \n",
    "        # [ ((index_of_sentence_in_input, index_of_similar_sentence_in_`dataset`), score), ...]\n",
    "        similar_texts_list = []\n",
    "        \n",
    "        for k in range(self.k):\n",
    "            score = sorted_scores[k]\n",
    "            new_tuple = (all_top_scores_dict[score], score)\n",
    "            similar_texts_list.append(new_tuple)\n",
    "\n",
    "        return similar_texts_list\n",
    "    \n",
    "    def convert_tag(self, tag):\n",
    "        \"\"\"Convert the tag given by nltk.pos_tag to the tag used by wordnet.synsets\"\"\"\n",
    "        tag_dict = {'N': 'n', 'J': 'a', 'R': 'r', 'V': 'v'}\n",
    "        try:\n",
    "            return tag_dict[tag[0]]\n",
    "        except KeyError:\n",
    "            return None\n",
    "\n",
    "    def doc_to_synsets(self, doc):\n",
    "        \"\"\"\n",
    "            Returns a list of synsets in document.\n",
    "            Tokenizes and tags the words in the document doc.\n",
    "            Then finds the first synset for each word/tag combination.\n",
    "        If a synset is not found for that combination it is skipped.\n",
    "\n",
    "        Args:\n",
    "            doc: string to be converted\n",
    "\n",
    "        Returns:\n",
    "            list of synsets\n",
    "        \"\"\"\n",
    "        tokens = word_tokenize(doc+' ')\n",
    "        \n",
    "        l = []\n",
    "        tags = nltk.pos_tag([tokens[0] + ' ']) if len(tokens) == 1 else nltk.pos_tag(tokens)\n",
    "        \n",
    "        for token, tag in zip(tokens, tags):\n",
    "            syntag = self.convert_tag(tag[1])\n",
    "            syns = wn.synsets(token, syntag)\n",
    "            if (len(syns) > 0):\n",
    "                l.append(syns[0])\n",
    "        return l  \n",
    "    \n",
    "    def similarity_score(self, s1, s2, distance_type = 'path'):\n",
    "        \"\"\"\n",
    "        Calculate the normalized similarity score of s1 onto s2\n",
    "        For each synset in s1, finds the synset in s2 with the largest similarity value.\n",
    "        Sum of all of the largest similarity values and normalize this value by dividing it by the\n",
    "        number of largest similarity values found.\n",
    "\n",
    "        Args:\n",
    "          s1, s2: list of synsets from doc_to_synsets\n",
    "\n",
    "        Returns:\n",
    "          normalized similarity score of s1 onto s2\n",
    "        \"\"\"\n",
    "        s1_largest_scores = []\n",
    "\n",
    "        for i, s1_synset in enumerate(s1, 0):\n",
    "            max_score = 0\n",
    "            for s2_synset in s2:\n",
    "                if distance_type == 'path':\n",
    "                    score = s1_synset.path_similarity(s2_synset, simulate_root = False)\n",
    "                else:\n",
    "                    score = s1_synset.wup_similarity(s2_synset)                  \n",
    "                if score != None:\n",
    "                    if score > max_score:\n",
    "                        max_score = score\n",
    "\n",
    "            if max_score != 0:\n",
    "                s1_largest_scores.append(max_score)\n",
    "\n",
    "        mean_score = np.mean(s1_largest_scores)\n",
    "\n",
    "        return mean_score \n",
    "    \n",
    "    def document_similarity(self, doc1, doc2):\n",
    "        \"\"\"Finds the similarity between doc1 and doc2\"\"\"\n",
    "\n",
    "        synsets1 = self.doc_to_synsets(doc1)\n",
    "        synsets2 = self.doc_to_synsets(doc2)\n",
    "          \n",
    "        return (self.similarity_score(synsets1, synsets2) + self.similarity_score(synsets2, synsets1)) / 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ab681c",
   "metadata": {},
   "source": [
    "## Single example (dryer sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9d1e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_value = 3\n",
    "preprocess = False\n",
    "\n",
    "classifier = KNN_Model(preprocess=preprocess, k=k_value, distance_type='path')\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f51ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = 'They are coated in toxic chemicals. Dryer sheets are one of the very worst things from a chemical allergy standpoint.'\n",
    "\n",
    "input_sentences = classifier.split_input(input_text)\n",
    "\n",
    "y_pred = classifier.predict(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1dd4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "# print(f\"Top {k_value} similar examples:\")\n",
    "\n",
    "unique_similar_sentences = []\n",
    "all_similar_sentences = []\n",
    "similar_sentence_to_original_sentence_dict = {}  # Value is a tuple like (original_sentence, score)\n",
    "\n",
    "# Print out the k most similar sentences (across all sentences in input)\n",
    "for i, result in enumerate (y_pred):\n",
    "    original_sentence_index = result[0][0][0]\n",
    "    \n",
    "    if original_sentence_index == len(input_sentences):\n",
    "        original_sentence = input_text\n",
    "    else:\n",
    "        original_sentence = input_sentences[original_sentence_index]\n",
    "    \n",
    "    similar_sentence_index = result[0][0][1]\n",
    "    similar_sentence_data = df.iloc[[similar_sentence_index]].values.tolist()[0]\n",
    "    \n",
    "    text_original_column_index = 11\n",
    "    label_categorical_column_index = 7\n",
    "    \n",
    "    score = result[1]\n",
    "    \n",
    "    similar_sentence = similar_sentence_data[text_original_column_index]\n",
    "    all_similar_sentences.append(similar_sentence)\n",
    "    \n",
    "    original_sentence_score_tuple = (original_sentence, score)\n",
    "    if similar_sentence in similar_sentence_to_original_sentence_dict:\n",
    "        similar_sentence_to_original_sentence_dict[similar_sentence].append(original_sentence_score_tuple)\n",
    "    else:\n",
    "        similar_sentence_to_original_sentence_dict[similar_sentence] = [original_sentence_score_tuple]\n",
    "        \n",
    "    if similar_sentence in unique_similar_sentences:\n",
    "        continue\n",
    "    else:\n",
    "        unique_similar_sentences.append(similar_sentence)\n",
    "    \n",
    "#     print(f'Original sentence: {original_sentence}')\n",
    "#     print(f'Similar sentence: {similar_sentence}')\n",
    "#     print(f'Label: {similar_sentence_data[label_categorical_column_index]}')\n",
    "#     print(f'Score: {score}')\n",
    "#     print()\n",
    "    \n",
    "similar_sentence_counter = Counter(all_similar_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5115c220",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common = similar_sentence_counter.most_common()\n",
    "\n",
    "print(\"Most common similar sentences for input text\")\n",
    "print()\n",
    "\n",
    "for (similar_sentence, count) in most_common:\n",
    "    print(f'SIMILAR SENTENCE: \\'{similar_sentence}\\'; COUNT: {count}')\n",
    "    original_sentence_score_tuple_list = similar_sentence_to_original_sentence_dict[similar_sentence]\n",
    "    \n",
    "    # Sort the tuples by the length of the first object in the tuple so that if the full input_text is \n",
    "    #    one of the similar sentences, it will be printed first\n",
    "    original_sentence_score_tuple_list.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    \n",
    "    print('ORIGINAL SENTENCES:')\n",
    "    for original_sentence_score_tuple in original_sentence_score_tuple_list:\n",
    "        original_sentence = original_sentence_score_tuple[0]\n",
    "        score = original_sentence_score_tuple[1]\n",
    "        if original_sentence == input_text:\n",
    "            print(f'   - [***FULL INPUT TEXT***] {original_sentence}  ({score})')\n",
    "        else:\n",
    "            print(f'   - {original_sentence}  ({score})')\n",
    "        \n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4ccc18",
   "metadata": {},
   "source": [
    "## Use examples from PUBHEALTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d2f4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(data_file_path):\n",
    "    if data_file_path.endswith('tsv'):\n",
    "        df = pd.read_csv(data_file_path, sep='\\t')\n",
    "    else:  # assume csv\n",
    "        df = pd.read_csv(data_file_path)\n",
    "\n",
    "    df, df_tags = format_tags(df)\n",
    "\n",
    "    mask = df['tags'].apply(lambda x: health(x))\n",
    "    df = df[mask]\n",
    "\n",
    "    # text_col contains the column name of where claims are found\n",
    "    # answer_col contains the column name of where post labels (true, false, etc.) are found\n",
    "    text_col = \"text\"\n",
    "    answer_col = \"label\"\n",
    "\n",
    "    # Rename the claim column to \"text\" and label column to \"label_categorical\"\n",
    "    df.rename(columns = {\"claim\": \"text\", \"label\": \"label_categorical\"}, inplace = True)\n",
    "    # Make the categorical labels into numbers (0, 1, 2, 3)\n",
    "    df[\"label\"] = pd.factorize(df[\"label_categorical\"])[0]\n",
    "    df = df.dropna(subset=[text_col])\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Make a copy of the 'text' column\n",
    "    df['text_original'] = df['text']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adebc6df",
   "metadata": {},
   "source": [
    "### Functions to process an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad6809b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input_text(input_text, classifier):\n",
    "    input_sentences = classifier.split_input(input_text)\n",
    "\n",
    "    y_pred = classifier.predict(input_text)\n",
    "    \n",
    "    unique_similar_sentences = []\n",
    "    all_similar_sentences = []\n",
    "    labels = []\n",
    "    similar_sentence_to_original_sentence_dict = {}  # Value is a tuple like (original_sentence, score)\n",
    "\n",
    "    # Print out the k most similar sentences (across all sentences in input)\n",
    "    for i, result in enumerate (y_pred):\n",
    "        original_sentence_index = result[0][0][0]\n",
    "\n",
    "        if original_sentence_index == len(input_sentences):\n",
    "            original_sentence = input_text\n",
    "        else:\n",
    "            original_sentence = input_sentences[original_sentence_index]\n",
    "\n",
    "        similar_sentence_index = result[0][0][1]\n",
    "        similar_sentence_data = df.iloc[[similar_sentence_index]].values.tolist()[0]\n",
    "\n",
    "        text_original_column_index = 11\n",
    "        label_categorical_column_index = 7\n",
    "#         label_column_index = \n",
    "\n",
    "        score = result[1]\n",
    "\n",
    "        similar_sentence = similar_sentence_data[text_original_column_index]\n",
    "        label = similar_sentence_data[label_categorical_column_index]\n",
    "        all_similar_sentences.append(similar_sentence)\n",
    "        labels.append(label)\n",
    "\n",
    "        original_sentence_score_tuple = (original_sentence, score)\n",
    "        if similar_sentence in similar_sentence_to_original_sentence_dict:\n",
    "            similar_sentence_to_original_sentence_dict[similar_sentence].append(original_sentence_score_tuple)\n",
    "        else:\n",
    "            similar_sentence_to_original_sentence_dict[similar_sentence] = [original_sentence_score_tuple]\n",
    "\n",
    "        if similar_sentence in unique_similar_sentences:\n",
    "            continue\n",
    "        else:\n",
    "            unique_similar_sentences.append(similar_sentence)\n",
    "\n",
    "    similar_sentence_counter = Counter(all_similar_sentences)\n",
    "    \n",
    "    most_common = similar_sentence_counter.most_common()\n",
    "\n",
    "    print(\" ~~~ Most common similar sentences for input text ~~~\")\n",
    "    print()\n",
    "\n",
    "    for (similar_sentence, count) in most_common:\n",
    "        print(f'SIMILAR SENTENCE: \\'{similar_sentence}\\'; LABEL: {label}; COUNT: {count}')\n",
    "        original_sentence_score_tuple_list = similar_sentence_to_original_sentence_dict[similar_sentence]\n",
    "\n",
    "        # Sort the tuples by the length of the first object in the tuple so that if the full input_text is \n",
    "        #    one of the similar sentences, it will be printed first\n",
    "        original_sentence_score_tuple_list.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073245a7",
   "metadata": {},
   "source": [
    "### 10 examples of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e599c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_file_path = \"/Users/gwenythportillowightman/OneDrive - Johns Hopkins/fall-2022/interpretable_ml_design/PUBHEALTH/test.tsv\"          \n",
    "\n",
    "test_df = prepare_df(test_data_file_path)\n",
    "\n",
    "test_data_subset = test_df[:10]\n",
    "\n",
    "for index, row in test_df_subset.iterrows():\n",
    "    input_text = row['text']\n",
    "    process_input_text(input_text, classifier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae652c8",
   "metadata": {},
   "source": [
    "### Examples from [User Study Examples](https://docs.google.com/spreadsheets/d/1BF-PR27TVwq9P6pcZQ9eHCOuQVP9nT989nmta5CbcGM/edit#gid=63935314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2353e42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier for user study examples\n",
    "k_value = 5\n",
    "preprocess = False\n",
    "\n",
    "classifier = KNN_Model(preprocess=preprocess, k=k_value, distance_type='path')\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3fba366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 3)\n",
      "------- Getting similar sentences for \"You can get HIV from fruits or vegetables from other countries.\" (1/1) ------\n",
      " ~~~ Most common similar sentences for input text ~~~\n",
      "\n",
      "SIMILAR SENTENCE: 'Straight-talking Fauci explains outbreak to a worried nation.'; LABEL: FALSE; COUNT: 1\n",
      "\n",
      "SIMILAR SENTENCE: 'Fruits and vegetables from other countries are being contaminated with blood containing either AIDS or HIV.'; LABEL: FALSE; COUNT: 1\n",
      "\n",
      "SIMILAR SENTENCE: 'Explainer: Why are some South Koreans who recovered from the coronavirus testing positive again?.'; LABEL: FALSE; COUNT: 1\n",
      "\n",
      "SIMILAR SENTENCE: 'Britain asks it citizens to help pick fruit and vegetables.'; LABEL: FALSE; COUNT: 1\n",
      "\n",
      "SIMILAR SENTENCE: 'There have been cases of imported fruits and vegetables that have been contaminated with HIV positive blood.'; LABEL: FALSE; COUNT: 1\n",
      "\n",
      "------- Getting similar sentences for \"Lead in candles can cause health issues.\" (1/1) ------\n",
      " ~~~ Most common similar sentences for input text ~~~\n",
      "\n",
      "SIMILAR SENTENCE: 'The lead in candles helps you reach your daily nutritional health goals!'; LABEL: unproven; COUNT: 1\n",
      "\n",
      "SIMILAR SENTENCE: 'Phelps honored for honesty on mental health, helping others.'; LABEL: unproven; COUNT: 1\n",
      "\n",
      "SIMILAR SENTENCE: ''Unprecedented' mental health issues seen in Hong Kong amid virus fears.'; LABEL: unproven; COUNT: 1\n",
      "\n",
      "SIMILAR SENTENCE: 'Celebrities bring awareness to mental health issues.'; LABEL: unproven; COUNT: 1\n",
      "\n",
      "SIMILAR SENTENCE: ''Clubbed' or curved fingernails are indicators of a serious underlying health issue.'; LABEL: unproven; COUNT: 1\n",
      "\n",
      "------- Getting similar sentences for \"Apple cider vinegar can help with everything including diets.\" (1/3) ------\n",
      "------- Getting similar sentences for \"It is great for losing weight.\" (2/3) ------\n",
      "------- Getting similar sentences for \"Apple cider vinegar can help with everything including diets. It is great for losing weight.\" (3/3) ------\n",
      " ~~~ Most common similar sentences for input text ~~~\n",
      "\n",
      "SIMILAR SENTENCE: '\"CNN aired a story about a university student who discovered a \"\"diet hack\"\" involving apple cider vinegar.\"'; LABEL: FALSE; COUNT: 2\n",
      "\n",
      "SIMILAR SENTENCE: 'Apple cider vinegar is a great way to lose weight. All you have to do is drink one tablespoon every morning.'; LABEL: FALSE; COUNT: 1\n",
      "\n",
      "SIMILAR SENTENCE: ' Drinking a few spoonfuls of Apple Cider Vinegar a day brings a host of health benefits. '; LABEL: FALSE; COUNT: 1\n",
      "\n",
      "SIMILAR SENTENCE: 'Apple cider vinegar mixed in water speeds up your metabolism and helps you keep the weight off'; LABEL: FALSE; COUNT: 1\n",
      "\n",
      "------- Getting similar sentences for \"They are coated in toxic chemicals.\" (1/3) ------\n",
      "------- Getting similar sentences for \"Dryer sheets are one of the very worst things from a chemical allergy standpoint.\" (2/3) ------\n",
      "------- Getting similar sentences for \"They are coated in toxic chemicals. Dryer sheets are one of the very worst things from a chemical allergy standpoint.\" (3/3) ------\n",
      " ~~~ Most common similar sentences for input text ~~~\n",
      "\n",
      "SIMILAR SENTENCE: 'Dryer sheets permeated with fabric softener contain at least seven dangerous toxic chemicals.'; LABEL: FALSE; COUNT: 3\n",
      "\n",
      "SIMILAR SENTENCE: 'English Channel dolphins carry ‘toxic cocktail’ of chemicals.'; LABEL: FALSE; COUNT: 1\n",
      "\n",
      "SIMILAR SENTENCE: 'Dryer sheets are full of toxic chemicals. Keep them away from your children if you care about them.'; LABEL: FALSE; COUNT: 1\n",
      "\n",
      "------- Getting similar sentences for \"Safflower oil and supplements can help you lose weight.\" (1/1) ------\n",
      " ~~~ Most common similar sentences for input text ~~~\n",
      "\n",
      "SIMILAR SENTENCE: 'A spoonful of safflower oil a day keeps the pounds away! Safflower oil or supplements are a great way to lose weight.'; LABEL: TRUE; COUNT: 1\n",
      "\n",
      "SIMILAR SENTENCE: 'Unlike marijuana, medical cannabis oil cannot get you high.'; LABEL: TRUE; COUNT: 1\n",
      "\n",
      "SIMILAR SENTENCE: 'Herbs, supplements often sold deceptively: U.S. report.'; LABEL: TRUE; COUNT: 1\n",
      "\n",
      "SIMILAR SENTENCE: 'Soup cans are lined with dangerous levels of BPA.'; LABEL: TRUE; COUNT: 1\n",
      "\n",
      "SIMILAR SENTENCE: 'Norovirus at assisted living facility affects early voting.'; LABEL: TRUE; COUNT: 1\n",
      "\n",
      "------- Getting similar sentences for \"Delayed release medication helps manage ADHD symptoms all day\" (1/1) ------\n",
      " ~~~ Most common similar sentences for input text ~~~\n",
      "\n",
      "SIMILAR SENTENCE: 'New delayed-release stimulant improves morning ADHD symptoms and all-day functioning'; LABEL: TRUE; COUNT: 1\n",
      "\n",
      "SIMILAR SENTENCE: 'Delayed-release ADHD medication provides all-day symptom control.'; LABEL: TRUE; COUNT: 1\n",
      "\n",
      "SIMILAR SENTENCE: 'When medication doesn't work: Innovative program eases ADHD symptoms naturally'; LABEL: TRUE; COUNT: 1\n",
      "\n",
      "SIMILAR SENTENCE: '“Man visited Albany, N.Y. days before dying from coronavirus.”'; LABEL: TRUE; COUNT: 1\n",
      "\n",
      "SIMILAR SENTENCE: 'Phelps honored for honesty on mental health, helping others.'; LABEL: TRUE; COUNT: 1\n",
      "\n",
      "------- Getting similar sentences for \"There have been no deaths due to vaping.\" (1/3) ------\n",
      "------- Getting similar sentences for \"E-cigarettes don't cause any harm, especially to your lungs!\" (2/3) ------\n",
      "------- Getting similar sentences for \"There have been no deaths due to vaping. E-cigarettes don't cause any harm, especially to your lungs!\" (3/3) ------\n",
      " ~~~ Most common similar sentences for input text ~~~\n",
      "\n",
      "SIMILAR SENTENCE: 'Fauci warns of ‘suffering and death’ if US reopens too soon.'; LABEL: TRUE; COUNT: 2\n",
      "\n",
      "SIMILAR SENTENCE: 'New Hampshire reports 1st vaping-related lung injury.'; LABEL: TRUE; COUNT: 1\n",
      "\n",
      "SIMILAR SENTENCE: 'Pennsylvania reports vaping death, investigating injuries.'; LABEL: TRUE; COUNT: 1\n",
      "\n",
      "SIMILAR SENTENCE: 'Larry King says he was operated on for lung cancer.'; LABEL: TRUE; COUNT: 1\n",
      "\n",
      "------- Getting similar sentences for \"Do not drink cold water after eating.\" (1/3) ------\n",
      "------- Getting similar sentences for \"Cold water after a meal can cause unpleasant health problems.\" (2/3) ------\n",
      "------- Getting similar sentences for \"Do not drink cold water after eating. Cold water after a meal can cause unpleasant health problems.\" (3/3) ------\n",
      " ~~~ Most common similar sentences for input text ~~~\n",
      "\n",
      "SIMILAR SENTENCE: 'Phelps honored for honesty on mental health, helping others.'; LABEL: FALSE; COUNT: 2\n",
      "\n",
      "SIMILAR SENTENCE: 'Drinking cold water after meals causes unpleasant and lasting side effects.'; LABEL: FALSE; COUNT: 2\n",
      "\n",
      "SIMILAR SENTENCE: 'The coronavirus is “simply the common cold.”'; LABEL: FALSE; COUNT: 1\n",
      "\n",
      "------- Getting similar sentences for \"It is all a lie created by big sunscreen!\" (1/3) ------\n",
      "------- Getting similar sentences for \"There is no link between sun exposure and skin cancer.\" (2/3) ------\n",
      "------- Getting similar sentences for \"It is all a lie created by big sunscreen! There is no link between sun exposure and skin cancer.\" (3/3) ------\n",
      " ~~~ Most common similar sentences for input text ~~~\n",
      "\n",
      "SIMILAR SENTENCE: 'A recent study disproved a link between sun exposure and skin cancer.'; LABEL: TRUE; COUNT: 2\n",
      "\n",
      "SIMILAR SENTENCE: 'Sun exposure actually makes you more resistant to the sun and helps you fight off skin cancer. its like a vaccine!'; LABEL: TRUE; COUNT: 2\n",
      "\n",
      "SIMILAR SENTENCE: 'Study finds possible link between sugary drinks and cancer.'; LABEL: TRUE; COUNT: 1\n",
      "\n",
      "------- Getting similar sentences for \"You can get medical marjuana if you tell the doctor you have anxiety.\" (1/1) ------\n",
      " ~~~ Most common similar sentences for input text ~~~\n",
      "\n",
      "SIMILAR SENTENCE: ''I thought I would never wake up,' Belgian doctor says after surviving COVID-19.'; LABEL: TRUE; COUNT: 1\n",
      "\n",
      "SIMILAR SENTENCE: 'No doctors who went to an American medical school will be accepting Obamacare.'; LABEL: TRUE; COUNT: 1\n",
      "\n",
      "SIMILAR SENTENCE: 'Unlike marijuana, medical cannabis oil cannot get you high.'; LABEL: TRUE; COUNT: 1\n",
      "\n",
      "SIMILAR SENTENCE: 'You cannot get (Ebola) from just riding on a plane or bus.'; LABEL: TRUE; COUNT: 1\n",
      "\n",
      "SIMILAR SENTENCE: 'Who gets the ventilator? British doctors contemplate harrowing coronavirus care choices.'; LABEL: TRUE; COUNT: 1\n",
      "\n",
      "------- Getting similar sentences for \"If you are in nature and run out of waer you can drink safely from any moving stream or river.\" (1/3) ------\n",
      "------- Getting similar sentences for \"Algae is just an extra vegetable!\" (2/3) ------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Getting similar sentences for \"If you are in nature and run out of waer you can drink safely from any moving stream or river. Algae is just an extra vegetable!\" (3/3) ------\n",
      " ~~~ Most common similar sentences for input text ~~~\n",
      "\n",
      "SIMILAR SENTENCE: '“The (COVID-19) cases are going up, but it's because the testing is going up.”'; LABEL: FALSE; COUNT: 2\n",
      "\n",
      "SIMILAR SENTENCE: 'Kansas officials issue toxic algae alert for Lake Afton.'; LABEL: FALSE; COUNT: 1\n",
      "\n",
      "SIMILAR SENTENCE: 'Philippines coronavirus testing to be stepped up soon: WHO.'; LABEL: FALSE; COUNT: 1\n",
      "\n",
      "SIMILAR SENTENCE: 'The coronavirus is “simply the common cold.”'; LABEL: FALSE; COUNT: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_study_examples_file_path = './user_study_examples.csv'\n",
    "\n",
    "USE_df = pd.read_csv(user_study_examples_file_path)\n",
    "print(f'{USE_df.shape}')\n",
    "\n",
    "for index, row in USE_df.iterrows():\n",
    "    input_text = row['Claims']\n",
    "    process_input_text(input_text, classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca75fb2",
   "metadata": {},
   "source": [
    "# Evaluation of PUBHEALTH test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ec3d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_file_path = \"./test.tsv\"          \n",
    "\n",
    "test_df = prepare_df(test_data_file_path)\n",
    "\n",
    "for index, row in test_df_subset.iterrows():\n",
    "    input_text = row['text']\n",
    "    process_input_text(input_text, classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}